{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "556d8425",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T17:05:32.110299Z",
     "iopub.status.busy": "2025-11-26T17:05:32.109650Z",
     "iopub.status.idle": "2025-11-26T17:08:18.585266Z",
     "shell.execute_reply": "2025-11-26T17:08:18.584523Z"
    },
    "papermill": {
     "duration": 166.483821,
     "end_time": "2025-11-26T17:08:18.587117",
     "exception": false,
     "start_time": "2025-11-26T17:05:32.103296",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/my-inference-packages-data/my_offline_packages\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/bitsandbytes-0.48.2-py3-none-manylinux_2_24_x86_64.whl\r\n",
      "Requirement already satisfied: torch<3,>=2.3 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (1.26.4)\r\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (25.0)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->bitsandbytes) (2.4.1)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (4.15.0)\r\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.3->bitsandbytes) (2025.10.0)\r\n",
      "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/torch-2.9.1-cp311-cp311-manylinux_2_28_x86_64.whl (from bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/sympy-1.14.0-py3-none-any.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cuda_nvrtc_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cuda_runtime_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cuda_cupti_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cudnn_cu12-9.10.2.21-py3-none-manylinux_2_27_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cublas_cu12-12.8.4.1-py3-none-manylinux_2_27_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cufft_cu12-11.3.3.83-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_curand_cu12-10.3.9.90-py3-none-manylinux_2_27_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cusolver_cu12-11.7.3.90-py3-none-manylinux_2_27_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cusparse_cu12-12.5.8.93-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cusparselt_cu12-0.7.1-py3-none-manylinux2014_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_nccl_cu12-2.27.5-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_nvshmem_cu12-3.3.20-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_nvtx_cu12-12.8.90-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_nvjitlink_cu12-12.8.93-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/nvidia_cufile_cu12-1.13.1.3-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Processing /kaggle/input/my-inference-packages-data/my_offline_packages/triton-3.5.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (from torch<3,>=2.3->bitsandbytes)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch<3,>=2.3->bitsandbytes) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.3->bitsandbytes) (3.0.3)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->bitsandbytes) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->bitsandbytes) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->bitsandbytes) (2024.2.0)\r\n",
      "Installing collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvshmem-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, bitsandbytes\r\n",
      "  Attempting uninstall: nvidia-cusparselt-cu12\r\n",
      "    Found existing installation: nvidia-cusparselt-cu12 0.6.2\r\n",
      "    Uninstalling nvidia-cusparselt-cu12-0.6.2:\r\n",
      "      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\r\n",
      "  Attempting uninstall: triton\r\n",
      "    Found existing installation: triton 3.2.0\r\n",
      "    Uninstalling triton-3.2.0:\r\n",
      "      Successfully uninstalled triton-3.2.0\r\n",
      "  Attempting uninstall: sympy\r\n",
      "    Found existing installation: sympy 1.13.1\r\n",
      "    Uninstalling sympy-1.13.1:\r\n",
      "      Successfully uninstalled sympy-1.13.1\r\n",
      "  Attempting uninstall: nvidia-nvtx-cu12\r\n",
      "    Found existing installation: nvidia-nvtx-cu12 12.4.127\r\n",
      "    Uninstalling nvidia-nvtx-cu12-12.4.127:\r\n",
      "      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\r\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\r\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-nccl-cu12\r\n",
      "    Found existing installation: nvidia-nccl-cu12 2.21.5\r\n",
      "    Uninstalling nvidia-nccl-cu12-2.21.5:\r\n",
      "      Successfully uninstalled nvidia-nccl-cu12-2.21.5\r\n",
      "  Attempting uninstall: nvidia-curand-cu12\r\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\r\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\r\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\r\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\r\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\r\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\r\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\r\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\r\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\r\n",
      "  Attempting uninstall: nvidia-cublas-cu12\r\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\r\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\r\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\r\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\r\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\r\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\r\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\r\n",
      "  Attempting uninstall: nvidia-cufft-cu12\r\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\r\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\r\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\r\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\r\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\r\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\r\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\r\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\r\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\r\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\r\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\r\n",
      "  Attempting uninstall: torch\r\n",
      "    Found existing installation: torch 2.6.0+cu124\r\n",
      "    Uninstalling torch-2.6.0+cu124:\r\n",
      "      Successfully uninstalled torch-2.6.0+cu124\r\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.9.1 which is incompatible.\r\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.9.1 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0mSuccessfully installed bitsandbytes-0.48.2 nvidia-cublas-cu12-12.8.4.1 nvidia-cuda-cupti-cu12-12.8.90 nvidia-cuda-nvrtc-cu12-12.8.93 nvidia-cuda-runtime-cu12-12.8.90 nvidia-cudnn-cu12-9.10.2.21 nvidia-cufft-cu12-11.3.3.83 nvidia-cufile-cu12-1.13.1.3 nvidia-curand-cu12-10.3.9.90 nvidia-cusolver-cu12-11.7.3.90 nvidia-cusparse-cu12-12.5.8.93 nvidia-cusparselt-cu12-0.7.1 nvidia-nccl-cu12-2.27.5 nvidia-nvjitlink-cu12-12.8.93 nvidia-nvshmem-cu12-3.3.20 nvidia-nvtx-cu12-12.8.90 sympy-1.14.0 torch-2.9.1 triton-3.5.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install bitsandbytes --no-index --find-links=/kaggle/input/my-inference-packages-data/my_offline_packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9a178c9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T17:08:18.597653Z",
     "iopub.status.busy": "2025-11-26T17:08:18.597035Z",
     "iopub.status.idle": "2025-11-26T17:08:23.005623Z",
     "shell.execute_reply": "2025-11-26T17:08:23.004878Z"
    },
    "papermill": {
     "duration": 4.415581,
     "end_time": "2025-11-26T17:08:23.007462",
     "exception": false,
     "start_time": "2025-11-26T17:08:18.591881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: /kaggle/input/torchvision0-24-1\r\n",
      "Processing /kaggle/input/torchvision0-24-1/torchvision-0.24.1-cp311-cp311-manylinux_2_28_x86_64.whl\r\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision==0.24.1) (1.26.4)\r\n",
      "Requirement already satisfied: torch==2.9.1 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.24.1) (2.9.1)\r\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision==0.24.1) (11.3.0)\r\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (3.20.0)\r\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (4.15.0)\r\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (1.14.0)\r\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (3.5)\r\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (3.1.6)\r\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (2025.10.0)\r\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (9.10.2.21)\r\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (12.8.4.1)\r\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (11.3.3.83)\r\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (10.3.9.90)\r\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (11.7.3.90)\r\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (12.5.8.93)\r\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (0.7.1)\r\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (2.27.5)\r\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (3.3.20)\r\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (12.8.90)\r\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (12.8.93)\r\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (1.13.1.3)\r\n",
      "Requirement already satisfied: triton==3.5.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.9.1->torchvision==0.24.1) (3.5.1)\r\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.1) (1.3.8)\r\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.1) (1.2.4)\r\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.1) (0.1.1)\r\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.1) (2025.3.0)\r\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.1) (2022.3.0)\r\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision==0.24.1) (2.4.1)\r\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch==2.9.1->torchvision==0.24.1) (1.3.0)\r\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.9.1->torchvision==0.24.1) (3.0.3)\r\n",
      "Requirement already satisfied: onemkl-license==2025.3.0 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.24.1) (2025.3.0)\r\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.24.1) (2024.2.0)\r\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision==0.24.1) (2022.3.0)\r\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision==0.24.1) (1.4.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision==0.24.1) (2024.2.0)\r\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision==0.24.1) (2024.2.0)\r\n",
      "Installing collected packages: torchvision\r\n",
      "  Attempting uninstall: torchvision\r\n",
      "    Found existing installation: torchvision 0.21.0+cu124\r\n",
      "    Uninstalling torchvision-0.21.0+cu124:\r\n",
      "      Successfully uninstalled torchvision-0.21.0+cu124\r\n",
      "Successfully installed torchvision-0.24.1\r\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision==0.24.1 --no-index --find-links=/kaggle/input/torchvision0-24-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11a8e1d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-26T17:08:23.020553Z",
     "iopub.status.busy": "2025-11-26T17:08:23.020288Z",
     "iopub.status.idle": "2025-11-26T17:09:55.453788Z",
     "shell.execute_reply": "2025-11-26T17:09:55.452923Z"
    },
    "papermill": {
     "duration": 92.441925,
     "end_time": "2025-11-26T17:09:55.455134",
     "exception": false,
     "start_time": "2025-11-26T17:08:23.013209",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 17:08:34.198303: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764176914.417894      20 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764176914.480922      20 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad9b73cf90bc4bafaa50ec62b5b9083f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "The following generation flags are not valid and may be ignored: ['cache_implementation']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "341ffa9ac0cd4742a929322f0ba2a1e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Protobuf Patch\n",
    "# ==========================================\n",
    "try:\n",
    "    from google.protobuf.message_factory import MessageFactory\n",
    "    if not hasattr(MessageFactory, 'GetPrototype'):\n",
    "        def get_prototype_replacement(self, descriptor):\n",
    "            try:\n",
    "                return self.GetMessageClass(descriptor)\n",
    "            except:\n",
    "                from google.protobuf.message import Message\n",
    "                class DefaultMessage(Message):\n",
    "                    def ParseFromString(self, s): pass\n",
    "                    def SerializeToString(self): return b\"\"\n",
    "                return DefaultMessage\n",
    "        MessageFactory.GetPrototype = get_prototype_replacement\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# ==========================================\n",
    "# Imports\n",
    "# ==========================================\n",
    "import os\n",
    "import json\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers.data.data_collator import pad_without_fast_tokenizer_warning\n",
    "from peft import PeftModel\n",
    "\n",
    "# ==========================================\n",
    "# Config\n",
    "# ==========================================\n",
    "TEST_CSV = '/kaggle/input/lmsys-chatbot-arena/test.csv'\n",
    "BASE_MODEL_PATH = \"/kaggle/input/gemma29b-base/transformers/default/1/gemma_base_model\"\n",
    "ADAPTER_PATH = \"/kaggle/input/gemma29b-lora/transformers/default/1/gemma_lora_final\"\n",
    "MAX_LEN = 1024\n",
    "BATCH_SIZE = 4\n",
    "TTA = True  # Test Time Augmentation\n",
    "\n",
    "# ==========================================\n",
    "# Helpers\n",
    "# ==========================================\n",
    "def process_text(text):\n",
    "    try:\n",
    "        text = str(text).replace(\"null\", \"'null'\")\n",
    "        parsed = json.loads(text)\n",
    "        if isinstance(parsed, list) and len(parsed) > 0:\n",
    "            return parsed[0]\n",
    "        return str(text)\n",
    "    except:\n",
    "        return str(text)\n",
    "\n",
    "def format_prompt(row, swap=False):\n",
    "    p = process_text(row['prompt'])[:512]\n",
    "    a = process_text(row['response_a'])[:1024]\n",
    "    b = process_text(row['response_b'])[:1024]\n",
    "    \n",
    "    if swap:\n",
    "        a, b = b, a  # Swap A and B\n",
    "    \n",
    "    return f\"<start_of_turn>user\\nWhich model's answer is better? Directly answer with 'A', 'B', or 'tie'.\\n\\n### Prompt\\n{p}\\n\\n### Response A\\n{a}\\n\\n### Response B\\n{b}<end_of_turn>\\n<start_of_turn>model\\n\"\n",
    "\n",
    "def get_probs(logits, tokenizer):\n",
    "    probs = torch.softmax(logits, dim=-1)\n",
    "    top_probs, top_ids = torch.topk(probs, 10)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(logits.shape[0]):\n",
    "        pa, pb, pt = 0.0, 0.0, 0.0\n",
    "        for p, idx in zip(top_probs[i].cpu().numpy(), top_ids[i].cpu().numpy()):\n",
    "            tok = tokenizer.decode([idx]).lower().strip().strip('.')\n",
    "            if tok == 'a': pa += p\n",
    "            elif tok == 'b': pb += p\n",
    "            elif tok in ['tie', 'draw']: pt += p\n",
    "        total = pa + pb + pt\n",
    "        if total < 1e-6:\n",
    "            results.append([0.33, 0.33, 0.34])\n",
    "        else:\n",
    "            results.append([pa/total, pb/total, pt/total])\n",
    "    return results\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.cuda.amp.autocast()\n",
    "def inference(df, model, tokenizer, device):\n",
    "    a_win, b_win, tie = [], [], []\n",
    "    \n",
    "    for i in range(0, len(df), BATCH_SIZE):\n",
    "        batch = df.iloc[i:i+BATCH_SIZE]\n",
    "        inputs = pad_without_fast_tokenizer_warning(\n",
    "            tokenizer,\n",
    "            {\"input_ids\": batch[\"input_ids\"].tolist(), \n",
    "             \"attention_mask\": batch[\"attention_mask\"].tolist()},\n",
    "            padding=\"longest\",\n",
    "            return_tensors=\"pt\",\n",
    "        ).to(device)\n",
    "        \n",
    "        logits = model(**inputs).logits[:, -1, :]\n",
    "        for p in get_probs(logits, tokenizer):\n",
    "            a_win.append(p[0])\n",
    "            b_win.append(p[1])\n",
    "            tie.append(p[2])\n",
    "    \n",
    "    df = df.copy()\n",
    "    df[\"winner_model_a\"] = a_win\n",
    "    df[\"winner_model_b\"] = b_win\n",
    "    df[\"winner_tie\"] = tie\n",
    "    return df\n",
    "\n",
    "def run_inference(test, tokenizer, model_0, model_1, swap=False):\n",
    "    \"\"\"Run inference with optional A/B swap\"\"\"\n",
    "    # Tokenize\n",
    "    prompts = test.apply(lambda row: format_prompt(row, swap=swap), axis=1).tolist()\n",
    "    tok = tokenizer(prompts, max_length=MAX_LEN, truncation=True, padding=False)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        \"id\": test[\"id\"],\n",
    "        \"input_ids\": tok.input_ids,\n",
    "        \"attention_mask\": tok.attention_mask,\n",
    "        \"length\": [len(x) for x in tok.input_ids]\n",
    "    })\n",
    "    \n",
    "    # Sort by length\n",
    "    data = data.sort_values(\"length\", ascending=False)\n",
    "    \n",
    "    # Split interleaved\n",
    "    sub_1 = data.iloc[0::2].copy()\n",
    "    sub_2 = data.iloc[1::2].copy()\n",
    "    \n",
    "    # Parallel inference\n",
    "    with ThreadPoolExecutor(max_workers=2) as ex:\n",
    "        results = list(ex.map(\n",
    "            inference,\n",
    "            [sub_1, sub_2],\n",
    "            [model_0, model_1],\n",
    "            [tokenizer, tokenizer],\n",
    "            [torch.device(\"cuda:0\"), torch.device(\"cuda:1\")]\n",
    "        ))\n",
    "    \n",
    "    result = pd.concat(results).sort_values(\"id\").reset_index(drop=True)\n",
    "    return result\n",
    "\n",
    "# ==========================================\n",
    "# Main\n",
    "# ==========================================\n",
    "# Load data\n",
    "test = pd.read_csv(TEST_CSV)\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "tokenizer.padding_side = \"left\"\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load models\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "model_0 = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH, quantization_config=bnb_config,\n",
    "    device_map={\"\": 0}, torch_dtype=torch.float16, use_cache=False\n",
    ")\n",
    "model_0 = PeftModel.from_pretrained(model_0, ADAPTER_PATH)\n",
    "model_0.eval()\n",
    "\n",
    "model_1 = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH, quantization_config=bnb_config,\n",
    "    device_map={\"\": 1}, torch_dtype=torch.float16, use_cache=False\n",
    ")\n",
    "model_1 = PeftModel.from_pretrained(model_1, ADAPTER_PATH)\n",
    "model_1.eval()\n",
    "\n",
    "# ==========================================\n",
    "# Original inference\n",
    "# ==========================================\n",
    "result_orig = run_inference(test, tokenizer, model_0, model_1, swap=False)\n",
    "\n",
    "if TTA:\n",
    "    # ==========================================\n",
    "    # TTA: Swap A/B and run again\n",
    "    # ==========================================\n",
    "    result_swap = run_inference(test, tokenizer, model_0, model_1, swap=True)\n",
    "    \n",
    "    # Swap 的結果要反轉回來：\n",
    "    # swap 版的 \"A wins\" 其實是原本的 \"B wins\"\n",
    "    # swap 版的 \"B wins\" 其實是原本的 \"A wins\"\n",
    "    # tie 不變\n",
    "    \n",
    "    orig_a = result_orig[\"winner_model_a\"].values\n",
    "    orig_b = result_orig[\"winner_model_b\"].values\n",
    "    orig_tie = result_orig[\"winner_tie\"].values\n",
    "    \n",
    "    swap_a = result_swap[\"winner_model_b\"].values  # swap 後 B 變成原本的 A\n",
    "    swap_b = result_swap[\"winner_model_a\"].values  # swap 後 A 變成原本的 B\n",
    "    swap_tie = result_swap[\"winner_tie\"].values\n",
    "    \n",
    "    # 平均兩次結果\n",
    "    final_a = (orig_a + swap_a) / 2\n",
    "    final_b = (orig_b + swap_b) / 2\n",
    "    final_tie = (orig_tie + swap_tie) / 2\n",
    "    \n",
    "    # Normalize to sum = 1\n",
    "    total = final_a + final_b + final_tie\n",
    "    final_a = final_a / total\n",
    "    final_b = final_b / total\n",
    "    final_tie = final_tie / total\n",
    "    \n",
    "    result = pd.DataFrame({\n",
    "        \"id\": result_orig[\"id\"],\n",
    "        \"winner_model_a\": final_a,\n",
    "        \"winner_model_b\": final_b,\n",
    "        \"winner_tie\": final_tie\n",
    "    })\n",
    "else:\n",
    "    result = result_orig[[\"id\", \"winner_model_a\", \"winner_model_b\", \"winner_tie\"]]\n",
    "\n",
    "# Save\n",
    "result.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9429bb0",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-23T16:04:40.529964Z",
     "iopub.status.busy": "2025-11-23T16:04:40.529406Z",
     "iopub.status.idle": "2025-11-23T16:06:39.593327Z",
     "shell.execute_reply": "2025-11-23T16:06:39.592465Z",
     "shell.execute_reply.started": "2025-11-23T16:04:40.529939Z"
    },
    "papermill": {
     "duration": 0.005119,
     "end_time": "2025-11-26T17:09:55.465915",
     "exception": false,
     "start_time": "2025-11-26T17:09:55.460796",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d715b2",
   "metadata": {
    "papermill": {
     "duration": 0.004761,
     "end_time": "2025-11-26T17:09:55.475554",
     "exception": false,
     "start_time": "2025-11-26T17:09:55.470793",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b91267",
   "metadata": {
    "papermill": {
     "duration": 0.004776,
     "end_time": "2025-11-26T17:09:55.485348",
     "exception": false,
     "start_time": "2025-11-26T17:09:55.480572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "isSourceIdPinned": false,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "datasetId": 8815025,
     "sourceId": 13840390,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 8838797,
     "sourceId": 13872965,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 512773,
     "modelInstanceId": 497441,
     "sourceId": 657984,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 512790,
     "modelInstanceId": 497458,
     "sourceId": 658003,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 270.0829,
   "end_time": "2025-11-26T17:09:58.392303",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-26T17:05:28.309403",
   "version": "2.6.0"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "111b8c924db64d4585e638167b8d0971": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "140b25069d4a447c925952e1038a60e9": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "341ffa9ac0cd4742a929322f0ba2a1e7": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_6ef9431df51c4ebcbec1c8c4b16cab73",
        "IPY_MODEL_f43723c2055645f0ab9a235f9ed5e810",
        "IPY_MODEL_f569f8d842904a29ad1fefcf88c8b296"
       ],
       "layout": "IPY_MODEL_6dbc22ed90ab4d988206304789f5a191",
       "tabbable": null,
       "tooltip": null
      }
     },
     "55d167ee5f1946f3b40041b79caf5f49": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6835c9f013984dfc83544d33f993b7a6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "6b9ecb4fd01e4bc6bdfde61db7116cc8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "6bcdd88fdbe6450a8454df7e9d354df2": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6c577faa65f14abf8e6f18dc629ad5f8": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_6bcdd88fdbe6450a8454df7e9d354df2",
       "placeholder": "​",
       "style": "IPY_MODEL_140b25069d4a447c925952e1038a60e9",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:44&lt;00:00, 19.86s/it]"
      }
     },
     "6dbc22ed90ab4d988206304789f5a191": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "6ef9431df51c4ebcbec1c8c4b16cab73": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_b4290217ee7c411d92fd4a0c88070b8d",
       "placeholder": "​",
       "style": "IPY_MODEL_6835c9f013984dfc83544d33f993b7a6",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "6f4e399a1d374ccf938156dcd9553ce6": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "77dcb10f731e4fc397458628a32af90d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "7a09853525c34a63b831fedc5e4b34b3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_111b8c924db64d4585e638167b8d0971",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_6b9ecb4fd01e4bc6bdfde61db7116cc8",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "7a10a28d73b941298ecd564534e65db0": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "87a8edfea1ab41ffaad615dcb3a99cd9": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "a54db92e10094426ab29aa55438f7c8e": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "background": null,
       "description_width": "",
       "font_size": null,
       "text_color": null
      }
     },
     "ad9b73cf90bc4bafaa50ec62b5b9083f": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_b8da15f6c4f24945a05cee04ebc28a12",
        "IPY_MODEL_7a09853525c34a63b831fedc5e4b34b3",
        "IPY_MODEL_6c577faa65f14abf8e6f18dc629ad5f8"
       ],
       "layout": "IPY_MODEL_6f4e399a1d374ccf938156dcd9553ce6",
       "tabbable": null,
       "tooltip": null
      }
     },
     "b4290217ee7c411d92fd4a0c88070b8d": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "2.0.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "2.0.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border_bottom": null,
       "border_left": null,
       "border_right": null,
       "border_top": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "justify_items": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "object_fit": null,
       "object_position": null,
       "order": null,
       "overflow": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "b8da15f6c4f24945a05cee04ebc28a12": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_55d167ee5f1946f3b40041b79caf5f49",
       "placeholder": "​",
       "style": "IPY_MODEL_a54db92e10094426ab29aa55438f7c8e",
       "tabbable": null,
       "tooltip": null,
       "value": "Loading checkpoint shards: 100%"
      }
     },
     "f43723c2055645f0ab9a235f9ed5e810": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "FloatProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "FloatProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_87a8edfea1ab41ffaad615dcb3a99cd9",
       "max": 2.0,
       "min": 0.0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_f46283ae9f9a442d81407a941746212d",
       "tabbable": null,
       "tooltip": null,
       "value": 2.0
      }
     },
     "f46283ae9f9a442d81407a941746212d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "2.0.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "f569f8d842904a29ad1fefcf88c8b296": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "2.0.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "2.0.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "2.0.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_allow_html": false,
       "layout": "IPY_MODEL_7a10a28d73b941298ecd564534e65db0",
       "placeholder": "​",
       "style": "IPY_MODEL_77dcb10f731e4fc397458628a32af90d",
       "tabbable": null,
       "tooltip": null,
       "value": " 2/2 [00:02&lt;00:00,  1.04s/it]"
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
