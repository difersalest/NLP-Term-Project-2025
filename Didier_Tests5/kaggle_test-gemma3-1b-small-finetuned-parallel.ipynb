{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-30T16:17:37.096429Z",
     "iopub.status.busy": "2025-11-30T16:17:37.095881Z",
     "iopub.status.idle": "2025-11-30T16:18:05.289059Z",
     "shell.execute_reply": "2025-11-30T16:18:05.288298Z",
     "shell.execute_reply.started": "2025-11-30T16:17:37.096407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 16:17:49.406683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764519469.560060      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764519469.605985      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import gc\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftConfig, PeftModelForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "BASE_MODEL_PATH = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\"\n",
    "ADAPTER_PATH = \"/kaggle/input/gemma-3-preference-adapter/transformers/v1/1\"\n",
    "TEST_CSV_PATH = \"/kaggle/input/lmsys-chatbot-arena/test.csv\"\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "# ==========================================\n",
    "# CUSTOM CLASSES & FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "class Gemma3ForSequenceClassification(PeftModelForSequenceClassification):\n",
    "    def __init__(self, peft_config: PeftConfig, model: AutoModelForCausalLM, adapter_name=\"default\"):\n",
    "        super().__init__(model, peft_config, adapter_name)\n",
    "        self.num_labels = model.config.num_labels\n",
    "        self.problem_type = \"single_label_classification\" \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.return_dict\n",
    "\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "\n",
    "        # Extract logits from the outputs\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Select last \"real\" token based on attention mask\n",
    "        sequence_lengths = torch.sum(attention_mask, dim=1)\n",
    "        last_token_indices = sequence_lengths - 1\n",
    "        batch_size = logits.shape[0]\n",
    "       \n",
    "        # Get the logits for the last token in the sequence\n",
    "        logits = logits[torch.arange(batch_size, device=logits.device), last_token_indices, :]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Logic mirroring your training code\n",
    "            if self.problem_type == \"regression\":\n",
    "                loss_fct = torch.nn.MSELoss()\n",
    "                loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "            elif self.problem_type == \"single_label_classification\":\n",
    "                loss_fct = torch.nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions)\n",
    "\n",
    "def prepare_inference_input(row, tokenizer, max_length=8192):\n",
    "    TEMPLATE_BUFFER = 200 \n",
    "    AVAILABLE_TOKENS = max_length - TEMPLATE_BUFFER\n",
    "    PROMPT_RATIO = 0.2\n",
    "    RESP_RATIO = 0.4\n",
    "\n",
    "    try:\n",
    "        prompt_text = \"\\n\".join(json.loads(row['prompt']))\n",
    "        resp_a_text = \"\\n\".join(json.loads(row['response_a']))\n",
    "        resp_b_text = \"\\n\".join(json.loads(row['response_b']))\n",
    "    except:\n",
    "        prompt_text = str(row['prompt'])\n",
    "        resp_a_text = str(row['response_a'])\n",
    "        resp_b_text = str(row['response_b'])\n",
    "\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)['input_ids']\n",
    "    resp_a_ids = tokenizer(resp_a_text, add_special_tokens=False)['input_ids']\n",
    "    resp_b_ids = tokenizer(resp_b_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    max_prompt_len = int(AVAILABLE_TOKENS * PROMPT_RATIO)\n",
    "    max_resp_len = int(AVAILABLE_TOKENS * RESP_RATIO)\n",
    "\n",
    "    if len(prompt_ids) > max_prompt_len: prompt_ids = prompt_ids[:max_prompt_len]\n",
    "    if len(resp_a_ids) > max_resp_len: resp_a_ids = resp_a_ids[-max_resp_len:] \n",
    "    if len(resp_b_ids) > max_resp_len: resp_b_ids = resp_b_ids[-max_resp_len:] \n",
    "\n",
    "    final_prompt = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "    final_resp_a = tokenizer.decode(resp_a_ids, skip_special_tokens=True)\n",
    "    final_resp_b = tokenizer.decode(resp_b_ids, skip_special_tokens=True)\n",
    "\n",
    "    return f\"\"\"# **Based on the following prompt choose which of the two responses you think humans would prefer the most:** \\\\n \n",
    "    ## **Prompt:**\n",
    "    `{final_prompt}`\\\\n\n",
    "    ## **Response A:**\n",
    "    `{final_resp_a}`\\\\n\n",
    "    ## **Response B:**\n",
    "    `{final_resp_b}`\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:21:53.589941Z",
     "iopub.status.busy": "2025-11-30T16:21:53.589550Z",
     "iopub.status.idle": "2025-11-30T16:21:53.605365Z",
     "shell.execute_reply": "2025-11-30T16:21:53.604624Z",
     "shell.execute_reply.started": "2025-11-30T16:21:53.589917Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PARALLEL SETUP\n",
    "# ==========================================\n",
    "\n",
    "import os\n",
    "# Disable internal tokenizer parallelism to avoid deadlocks within the threads\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "\n",
    "# Shared Tokenizer Logic\n",
    "try:\n",
    "    TOKENIZER_SOURCE = ADAPTER_PATH\n",
    "    # Verify we can load it\n",
    "    AutoTokenizer.from_pretrained(TOKENIZER_SOURCE, trust_remote_code=True)\n",
    "except:\n",
    "    TOKENIZER_SOURCE = BASE_MODEL_PATH\n",
    "\n",
    "# Function to Load a Model Replica on a specific GPU\n",
    "def load_model_on_device(device_id):\n",
    "    device_name = f\"cuda:{device_id}\"\n",
    "    print(f\"Loading Model Replica on {device_name}...\")\n",
    "    \n",
    "    # Load Base\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_PATH, \n",
    "        torch_dtype=torch.bfloat16,\n",
    "        device_map=device_name,\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    \n",
    "    # Init Head\n",
    "    base_model.lm_head = torch.nn.Linear(\n",
    "        base_model.config.hidden_size, 3, bias=False, device=device_name\n",
    "    ).to(torch.bfloat16)\n",
    "\n",
    "    # Load Adapter\n",
    "    peft_config = PeftConfig.from_pretrained(ADAPTER_PATH)\n",
    "    model = Gemma3ForSequenceClassification(peft_config, base_model)\n",
    "    model.load_adapter(ADAPTER_PATH, adapter_name=\"default\")\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# Worker Function for Threading\n",
    "def inference_worker(model, subset_df, device_id):\n",
    "    device_name = f\"cuda:{device_id}\"\n",
    "    \n",
    "    # Load a fresh tokenizer for this specific thread\n",
    "    local_tokenizer = AutoTokenizer.from_pretrained(\n",
    "        TOKENIZER_SOURCE, \n",
    "        padding_side='right', \n",
    "        add_bos=True, \n",
    "        trust_remote_code=True\n",
    "    )\n",
    "    if local_tokenizer.pad_token is None:\n",
    "        local_tokenizer.pad_token = local_tokenizer.eos_token\n",
    "\n",
    "    \n",
    "    print(f\"Worker {device_id}: Starting inference on {len(subset_df)} samples...\")\n",
    "    \n",
    "    # Format inputs using the LOCAL tokenizer\n",
    "    formatted_texts = []\n",
    "    for _, row in subset_df.iterrows():\n",
    "        # Pass local_tokenizer to your helper function\n",
    "        formatted_texts.append(prepare_inference_input(row, local_tokenizer))\n",
    "        \n",
    "    local_probs = []\n",
    "    BATCH_SIZE = 4 \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(formatted_texts), BATCH_SIZE), desc=f\"GPU {device_id}\", position=device_id):\n",
    "            batch_texts = formatted_texts[i : i + BATCH_SIZE]\n",
    "            \n",
    "            # Tokenize using LOCAL tokenizer\n",
    "            inputs = local_tokenizer(\n",
    "                batch_texts,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True,\n",
    "                truncation=True,\n",
    "                max_length=8192\n",
    "            ).to(device_name)\n",
    "            \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            # Safe Softmax\n",
    "            probs = F.softmax(logits.float(), dim=-1).cpu().numpy()\n",
    "            local_probs.extend(probs)\n",
    "            \n",
    "    return local_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:22:04.032537Z",
     "iopub.status.busy": "2025-11-30T16:22:04.032190Z",
     "iopub.status.idle": "2025-11-30T16:22:13.818552Z",
     "shell.execute_reply": "2025-11-30T16:22:13.817780Z",
     "shell.execute_reply.started": "2025-11-30T16:22:04.032504Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 2 GPUs found. Initializing dual-model setup.\n",
      "Loading Model Replica on cuda:0...\n",
      "Loading Model Replica on cuda:1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data split into 2 chunks.\n",
      "Worker 1: Starting inference on 1 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU 1:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker 0: Starting inference on 2 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU 0: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n",
      "\n",
      "GPU 1: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\u001b[A\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# EXECUTION\n",
    "# ==========================================\n",
    "\n",
    "# Check GPUs\n",
    "if torch.cuda.device_count() < 2:\n",
    "    print(\"⚠️ WARNING: Less than 2 GPUs found. Using Single GPU Mode on cuda:0\")\n",
    "    models = [load_model_on_device(0)]\n",
    "    gpu_indices = [0]\n",
    "else:\n",
    "    print(\"✅ 2 GPUs found. Initializing dual-model setup.\")\n",
    "    model_0 = load_model_on_device(0)\n",
    "    model_1 = load_model_on_device(1)\n",
    "    models = [model_0, model_1]\n",
    "    gpu_indices = [0, 1]\n",
    "\n",
    "# Load Data\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "ids = test_df['id'].tolist()\n",
    "\n",
    "# Split Data\n",
    "chunks = np.array_split(test_df, len(models))\n",
    "print(f\"Data split into {len(chunks)} chunks.\")\n",
    "\n",
    "# Run Parallel Inference\n",
    "results = []\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=len(models)) as executor:\n",
    "    futures = []\n",
    "    for i, model in enumerate(models):\n",
    "        futures.append(\n",
    "            executor.submit(inference_worker, model, chunks[i], gpu_indices[i])\n",
    "        )\n",
    "    \n",
    "    for future in futures:\n",
    "        results.extend(future.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:22:17.617930Z",
     "iopub.status.busy": "2025-11-30T16:22:17.617581Z",
     "iopub.status.idle": "2025-11-30T16:22:17.635593Z",
     "shell.execute_reply": "2025-11-30T16:22:17.634884Z",
     "shell.execute_reply.started": "2025-11-30T16:22:17.617897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n",
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060        0.435424        0.033552    0.531024\n",
      "1   211333        0.307735        0.022732    0.669534\n",
      "2  1233961        0.001448        0.994020    0.004532\n",
      "Saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SUBMISSION\n",
    "# ==========================================\n",
    "\n",
    "print(\"Creating submission file...\")\n",
    "probs_array = np.array(results)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'winner_model_a': probs_array[:, 0],\n",
    "    'winner_model_b': probs_array[:, 1],\n",
    "    'winner_tie': probs_array[:, 2]\n",
    "})\n",
    "\n",
    "print(submission.head())\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(f\"Saved to {SUBMISSION_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "isSourceIdPinned": false,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 222398,
     "modelInstanceId": 239467,
     "sourceId": 282742,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 520100,
     "modelInstanceId": 505198,
     "sourceId": 667311,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
