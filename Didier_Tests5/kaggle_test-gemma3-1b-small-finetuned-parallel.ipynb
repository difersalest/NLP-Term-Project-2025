{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":66631,"databundleVersionId":8346466,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":282742,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":239467,"modelId":222398},{"sourceId":667311,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":505198,"modelId":520100}],"dockerImageVersionId":31193,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport json\nimport torch\nimport pandas as pd\nimport numpy as np\nimport torch.nn.functional as F\nfrom tqdm import tqdm\nimport concurrent.futures\nimport gc\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig\n)\nfrom peft import PeftConfig, PeftModelForSequenceClassification\nfrom transformers.modeling_outputs import SequenceClassifierOutput\n\n# ==========================================\n# CONFIGURATION\n# ==========================================\n\nBASE_MODEL_PATH = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\"\nADAPTER_PATH = \"/kaggle/input/gemma-3-preference-adapter/transformers/v1/1\"\nTEST_CSV_PATH = \"/kaggle/input/lmsys-chatbot-arena/test.csv\"\nSUBMISSION_PATH = \"submission.csv\"\n\n# ==========================================\n# CUSTOM CLASSES & FUNCTIONS\n# ==========================================\n\nclass Gemma3ForSequenceClassification(PeftModelForSequenceClassification):\n    def __init__(self, peft_config: PeftConfig, model: AutoModelForCausalLM, adapter_name=\"default\"):\n        super().__init__(model, peft_config, adapter_name)\n        self.num_labels = model.config.num_labels\n        self.problem_type = \"multi_label_classification\" \n\n    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n        logits = outputs.logits\n        # Select last \"real\" token based on attention mask\n        sequence_lengths = torch.sum(attention_mask, dim=1)\n        last_token_indices = sequence_lengths - 1\n        batch_size = logits.shape[0]\n        logits = logits[torch.arange(batch_size, device=logits.device), last_token_indices, :]\n        return SequenceClassifierOutput(logits=logits)\n\ndef prepare_inference_input(row, tokenizer, max_length=8192):\n    TEMPLATE_BUFFER = 200 \n    AVAILABLE_TOKENS = max_length - TEMPLATE_BUFFER\n    PROMPT_RATIO = 0.2\n    RESP_RATIO = 0.4\n\n    try:\n        prompt_text = \"\\n\".join(json.loads(row['prompt']))\n        resp_a_text = \"\\n\".join(json.loads(row['response_a']))\n        resp_b_text = \"\\n\".join(json.loads(row['response_b']))\n    except:\n        prompt_text = str(row['prompt'])\n        resp_a_text = str(row['response_a'])\n        resp_b_text = str(row['response_b'])\n\n    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)['input_ids']\n    resp_a_ids = tokenizer(resp_a_text, add_special_tokens=False)['input_ids']\n    resp_b_ids = tokenizer(resp_b_text, add_special_tokens=False)['input_ids']\n\n    max_prompt_len = int(AVAILABLE_TOKENS * PROMPT_RATIO)\n    max_resp_len = int(AVAILABLE_TOKENS * RESP_RATIO)\n\n    if len(prompt_ids) > max_prompt_len: prompt_ids = prompt_ids[:max_prompt_len]\n    if len(resp_a_ids) > max_resp_len: resp_a_ids = resp_a_ids[-max_resp_len:] \n    if len(resp_b_ids) > max_resp_len: resp_b_ids = resp_b_ids[-max_resp_len:] \n\n    final_prompt = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n    final_resp_a = tokenizer.decode(resp_a_ids, skip_special_tokens=True)\n    final_resp_b = tokenizer.decode(resp_b_ids, skip_special_tokens=True)\n\n    return f\"\"\"# **Based on the following prompt choose which of the two responses you think humans would prefer the most:** \\\\n \n    ## **Prompt:**\n    `{final_prompt}`\\\\n\n    ## **Response A:**\n    `{final_resp_a}`\\\\n\n    ## **Response B:**\n    `{final_resp_b}`\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:17:37.095881Z","iopub.execute_input":"2025-11-30T16:17:37.096429Z","iopub.status.idle":"2025-11-30T16:18:05.289059Z","shell.execute_reply.started":"2025-11-30T16:17:37.096407Z","shell.execute_reply":"2025-11-30T16:18:05.288298Z"}},"outputs":[{"name":"stderr","text":"2025-11-30 16:17:49.406683: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1764519469.560060      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1764519469.605985      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"}],"execution_count":1},{"cell_type":"code","source":"# ==========================================\n# PARALLEL SETUP\n# ==========================================\n\nimport os\n# Disable internal tokenizer parallelism to avoid deadlocks within the threads\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n\n# Shared Tokenizer Logic\ntry:\n    TOKENIZER_SOURCE = ADAPTER_PATH\n    # Verify we can load it\n    AutoTokenizer.from_pretrained(TOKENIZER_SOURCE, trust_remote_code=True)\nexcept:\n    TOKENIZER_SOURCE = BASE_MODEL_PATH\n\n# Function to Load a Model Replica on a specific GPU\ndef load_model_on_device(device_id):\n    device_name = f\"cuda:{device_id}\"\n    print(f\"Loading Model Replica on {device_name}...\")\n    \n    # Load Base\n    base_model = AutoModelForCausalLM.from_pretrained(\n        BASE_MODEL_PATH, \n        torch_dtype=torch.bfloat16,\n        device_map=device_name,\n        trust_remote_code=True\n    )\n    \n    # Init Head\n    base_model.lm_head = torch.nn.Linear(\n        base_model.config.hidden_size, 3, bias=False, device=device_name\n    ).to(torch.bfloat16)\n\n    # Load Adapter\n    peft_config = PeftConfig.from_pretrained(ADAPTER_PATH)\n    model = Gemma3ForSequenceClassification(peft_config, base_model)\n    model.load_adapter(ADAPTER_PATH, adapter_name=\"default\")\n    model.eval()\n    return model\n\n# Worker Function for Threading\ndef inference_worker(model, subset_df, device_id):\n    device_name = f\"cuda:{device_id}\"\n    \n    # Load a fresh tokenizer for this specific thread\n    local_tokenizer = AutoTokenizer.from_pretrained(\n        TOKENIZER_SOURCE, \n        padding_side='right', \n        add_bos=True, \n        trust_remote_code=True\n    )\n    if local_tokenizer.pad_token is None:\n        local_tokenizer.pad_token = local_tokenizer.eos_token\n\n    \n    print(f\"Worker {device_id}: Starting inference on {len(subset_df)} samples...\")\n    \n    # Format inputs using the LOCAL tokenizer\n    formatted_texts = []\n    for _, row in subset_df.iterrows():\n        # Pass local_tokenizer to your helper function\n        formatted_texts.append(prepare_inference_input(row, local_tokenizer))\n        \n    local_probs = []\n    BATCH_SIZE = 4 \n    \n    with torch.no_grad():\n        for i in tqdm(range(0, len(formatted_texts), BATCH_SIZE), desc=f\"GPU {device_id}\", position=device_id):\n            batch_texts = formatted_texts[i : i + BATCH_SIZE]\n            \n            # Tokenize using LOCAL tokenizer\n            inputs = local_tokenizer(\n                batch_texts,\n                return_tensors=\"pt\",\n                padding=True,\n                truncation=True,\n                max_length=8192\n            ).to(device_name)\n            \n            outputs = model(**inputs)\n            logits = outputs.logits\n            \n            # Safe Softmax\n            probs = F.softmax(logits.float(), dim=-1).cpu().numpy()\n            local_probs.extend(probs)\n            \n    return local_probs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:21:53.589550Z","iopub.execute_input":"2025-11-30T16:21:53.589941Z","iopub.status.idle":"2025-11-30T16:21:53.605365Z","shell.execute_reply.started":"2025-11-30T16:21:53.589917Z","shell.execute_reply":"2025-11-30T16:21:53.604624Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ==========================================\n# EXECUTION\n# ==========================================\n\n# Check GPUs\nif torch.cuda.device_count() < 2:\n    print(\"⚠️ WARNING: Less than 2 GPUs found. Using Single GPU Mode on cuda:0\")\n    models = [load_model_on_device(0)]\n    gpu_indices = [0]\nelse:\n    print(\"✅ 2 GPUs found. Initializing dual-model setup.\")\n    model_0 = load_model_on_device(0)\n    model_1 = load_model_on_device(1)\n    models = [model_0, model_1]\n    gpu_indices = [0, 1]\n\n# Load Data\ntest_df = pd.read_csv(TEST_CSV_PATH)\nids = test_df['id'].tolist()\n\n# Split Data\nchunks = np.array_split(test_df, len(models))\nprint(f\"Data split into {len(chunks)} chunks.\")\n\n# Run Parallel Inference\nresults = []\nwith concurrent.futures.ThreadPoolExecutor(max_workers=len(models)) as executor:\n    futures = []\n    for i, model in enumerate(models):\n        futures.append(\n            executor.submit(inference_worker, model, chunks[i], gpu_indices[i])\n        )\n    \n    for future in futures:\n        results.extend(future.result())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:22:04.032190Z","iopub.execute_input":"2025-11-30T16:22:04.032537Z","iopub.status.idle":"2025-11-30T16:22:13.818552Z","shell.execute_reply.started":"2025-11-30T16:22:04.032504Z","shell.execute_reply":"2025-11-30T16:22:13.817780Z"}},"outputs":[{"name":"stdout","text":"✅ 2 GPUs found. Initializing dual-model setup.\nLoading Model Replica on cuda:0...\nLoading Model Replica on cuda:1...\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n  return bound(*args, **kwds)\n","output_type":"stream"},{"name":"stdout","text":"Data split into 2 chunks.\nWorker 1: Starting inference on 1 samples...\n","output_type":"stream"},{"name":"stderr","text":"\nGPU 1:   0%|          | 0/1 [00:00<?, ?it/s]\u001b[A","output_type":"stream"},{"name":"stdout","text":"Worker 0: Starting inference on 2 samples...\n","output_type":"stream"},{"name":"stderr","text":"GPU 0: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s]\n\nGPU 1: 100%|██████████| 1/1 [00:01<00:00,  1.42s/it]\u001b[A\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==========================================\n# SUBMISSION\n# ==========================================\n\nprint(\"Creating submission file...\")\nprobs_array = np.array(results)\n\nsubmission = pd.DataFrame({\n    'id': ids,\n    'winner_model_a': probs_array[:, 0],\n    'winner_model_b': probs_array[:, 1],\n    'winner_tie': probs_array[:, 2]\n})\n\nprint(submission.head())\nsubmission.to_csv(SUBMISSION_PATH, index=False)\nprint(f\"Saved to {SUBMISSION_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T16:22:17.617581Z","iopub.execute_input":"2025-11-30T16:22:17.617930Z","iopub.status.idle":"2025-11-30T16:22:17.635593Z","shell.execute_reply.started":"2025-11-30T16:22:17.617897Z","shell.execute_reply":"2025-11-30T16:22:17.634884Z"}},"outputs":[{"name":"stdout","text":"Creating submission file...\n        id  winner_model_a  winner_model_b  winner_tie\n0   136060        0.435424        0.033552    0.531024\n1   211333        0.307735        0.022732    0.669534\n2  1233961        0.001448        0.994020    0.004532\nSaved to submission.csv\n","output_type":"stream"}],"execution_count":6}]}