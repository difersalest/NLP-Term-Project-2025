{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-30T15:47:33.689433Z",
     "iopub.status.busy": "2025-11-30T15:47:33.688544Z",
     "iopub.status.idle": "2025-11-30T15:48:23.563326Z",
     "shell.execute_reply": "2025-11-30T15:48:23.562359Z",
     "shell.execute_reply.started": "2025-11-30T15:47:33.689394Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-30 15:47:53.426492: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1764517673.812730      47 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1764517673.920564      47 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig\n",
    ")\n",
    "from peft import PeftConfig, PeftModelForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "# ==========================================\n",
    "# CONFIGURATION\n",
    "# ==========================================\n",
    "\n",
    "# Base Model Path\n",
    "BASE_MODEL_PATH = \"/kaggle/input/gemma-3/transformers/gemma-3-1b-it/1\"\n",
    "\n",
    "# Adapter Path\n",
    "ADAPTER_PATH = \"/kaggle/input/gemma-3-preference-adapter/transformers/v1/1\" \n",
    "\n",
    "# Data Paths\n",
    "TEST_CSV_PATH = \"/kaggle/input/lmsys-chatbot-arena/test.csv\"\n",
    "SUBMISSION_PATH = \"submission.csv\"\n",
    "\n",
    "# Device\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# ==========================================\n",
    "# CUSTOM CLASSES & FUNCTIONS\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "class Gemma3ForSequenceClassification(PeftModelForSequenceClassification):\n",
    "    def __init__(self, peft_config: PeftConfig, model: AutoModelForCausalLM, adapter_name=\"default\"):\n",
    "        super().__init__(model, peft_config, adapter_name)\n",
    "        self.num_labels = model.config.num_labels\n",
    "        self.problem_type = \"single_label_classification\" \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.return_dict\n",
    "\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "\n",
    "        # Extract logits from the outputs\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Select last \"real\" token based on attention mask\n",
    "        sequence_lengths = torch.sum(attention_mask, dim=1)\n",
    "        last_token_indices = sequence_lengths - 1\n",
    "        batch_size = logits.shape[0]\n",
    "       \n",
    "        # Get the logits for the last token in the sequence\n",
    "        logits = logits[torch.arange(batch_size, device=logits.device), last_token_indices, :]\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Logic mirroring your training code\n",
    "            if self.problem_type == \"regression\":\n",
    "                loss_fct = torch.nn.MSELoss()\n",
    "                loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "            elif self.problem_type == \"single_label_classification\":\n",
    "                loss_fct = torch.nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions)\n",
    "\n",
    "# Helper to Format Input (Sandwich Strategy)\n",
    "def prepare_inference_input(row, tokenizer, max_length=8192):\n",
    "    # Configuration from training\n",
    "    TEMPLATE_BUFFER = 200 \n",
    "    AVAILABLE_TOKENS = max_length - TEMPLATE_BUFFER\n",
    "    PROMPT_RATIO = 0.2\n",
    "    RESP_RATIO = 0.4\n",
    "\n",
    "    # Parse JSON strings to actual text\n",
    "    try:\n",
    "        prompt_text = \"\\n\".join(json.loads(row['prompt']))\n",
    "        resp_a_text = \"\\n\".join(json.loads(row['response_a']))\n",
    "        resp_b_text = \"\\n\".join(json.loads(row['response_b']))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        prompt_text = str(row['prompt'])\n",
    "        resp_a_text = str(row['response_a'])\n",
    "        resp_b_text = str(row['response_b'])\n",
    "\n",
    "    # Tokenize to check lengths\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)['input_ids']\n",
    "    resp_a_ids = tokenizer(resp_a_text, add_special_tokens=False)['input_ids']\n",
    "    resp_b_ids = tokenizer(resp_b_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # Apply Budget (Sandwich Logic)\n",
    "    max_prompt_len = int(AVAILABLE_TOKENS * PROMPT_RATIO)\n",
    "    max_resp_len = int(AVAILABLE_TOKENS * RESP_RATIO)\n",
    "\n",
    "    # Prompt: Keep Start\n",
    "    if len(prompt_ids) > max_prompt_len:\n",
    "        prompt_ids = prompt_ids[:max_prompt_len]\n",
    "\n",
    "    # Responses: Keep End\n",
    "    if len(resp_a_ids) > max_resp_len:\n",
    "        resp_a_ids = resp_a_ids[-max_resp_len:] \n",
    "    \n",
    "    if len(resp_b_ids) > max_resp_len:\n",
    "        resp_b_ids = resp_b_ids[-max_resp_len:] \n",
    "\n",
    "    # Decode back to text\n",
    "    final_prompt = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "    final_resp_a = tokenizer.decode(resp_a_ids, skip_special_tokens=True)\n",
    "    final_resp_b = tokenizer.decode(resp_b_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Construct the Final Formatted String\n",
    "    return f\"\"\"# **Based on the following prompt choose which of the two responses you think humans would prefer the most:** \\\\n \n",
    "    ## **Prompt:**\n",
    "    `{final_prompt}`\\\\n\n",
    "    ## **Response A:**\n",
    "    `{final_resp_a}`\\\\n\n",
    "    ## **Response B:**\n",
    "    `{final_resp_b}`\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:00:37.049233Z",
     "iopub.status.busy": "2025-11-30T16:00:37.048273Z",
     "iopub.status.idle": "2025-11-30T16:01:18.070465Z",
     "shell.execute_reply": "2025-11-30T16:01:18.069796Z",
     "shell.execute_reply.started": "2025-11-30T16:00:37.049186Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Tokenizer...\n",
      "Loading Base Model (4-bit)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resizing lm_head to 3...\n",
      "Loading Adapter...\n",
      "Model ready!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# LOAD MODEL\n",
    "# ==========================================\n",
    "\n",
    "print(\"Loading Tokenizer...\")\n",
    "# Try loading tokenizer from adapter first, else base model\n",
    "try:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ADAPTER_PATH, padding_side='right', add_bos=True, trust_remote_code=True)\n",
    "except:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, padding_side='right', add_bos=True, trust_remote_code=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Loading Base Model (4-bit)...\")\n",
    "# Matching training config\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_use_double_quant=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    BASE_MODEL_PATH, \n",
    "    # quantization_config=bnb_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=DEVICE,\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "# Initialize Classification Head (3 classes)\n",
    "num_labels = 3\n",
    "print(f\"Resizing lm_head to {num_labels}...\")\n",
    "base_model.lm_head = torch.nn.Linear(\n",
    "    base_model.config.hidden_size,\n",
    "    num_labels,\n",
    "    bias=False,\n",
    "    device=DEVICE\n",
    ").to(torch.bfloat16)\n",
    "\n",
    "print(\"Loading Adapter...\")\n",
    "peft_config = PeftConfig.from_pretrained(ADAPTER_PATH)\n",
    "\n",
    "# Wrap base model in custom class\n",
    "model = Gemma3ForSequenceClassification(peft_config, base_model)\n",
    "\n",
    "# Load adapter weights\n",
    "# This will also load the trained lm_head weights because 'modules_to_save' included it\n",
    "model.load_adapter(ADAPTER_PATH, adapter_name=\"default\")\n",
    "model.eval()\n",
    "\n",
    "print(\"Model ready!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:04:08.611278Z",
     "iopub.status.busy": "2025-11-30T16:04:08.610976Z",
     "iopub.status.idle": "2025-11-30T16:04:12.415520Z",
     "shell.execute_reply": "2025-11-30T16:04:12.414724Z",
     "shell.execute_reply.started": "2025-11-30T16:04:08.611260Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total samples: 3\n",
      "Formatting inputs (Sandwich Strategy)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 314.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.78s/it]\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# INFERENCE\n",
    "# ==========================================\n",
    "\n",
    "# Load Data\n",
    "test_df = pd.read_csv(TEST_CSV_PATH)\n",
    "ids = test_df['id'].tolist()\n",
    "print(f\"Total samples: {len(test_df)}\")\n",
    "\n",
    "# Pre-format inputs\n",
    "print(\"Formatting inputs (Sandwich Strategy)...\")\n",
    "formatted_texts = []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    formatted_texts.append(prepare_inference_input(row, tokenizer))\n",
    "\n",
    "# Run Inference\n",
    "BATCH_SIZE = 4 # Adjust based on GPU memory\n",
    "all_probs = []\n",
    "\n",
    "print(\"Running inference...\")\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(formatted_texts), BATCH_SIZE)):\n",
    "        batch_texts = formatted_texts[i : i + BATCH_SIZE]\n",
    "        \n",
    "        inputs = tokenizer(\n",
    "            batch_texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=8192\n",
    "        ).to(DEVICE)\n",
    "        \n",
    "        outputs = model(**inputs)\n",
    "        \n",
    "        # Handle tuple vs object return\n",
    "        if isinstance(outputs, tuple):\n",
    "            logits = outputs[0]\n",
    "        else:\n",
    "            logits = outputs.logits\n",
    "            \n",
    "        # Convert Logits to Probabilities (Softmax)\n",
    "        # probs = F.softmax(logits, dim=-1).cpu().numpy()\n",
    "        probs = F.softmax(logits.float(), dim=-1).cpu().numpy()\n",
    "        all_probs.extend(probs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-30T16:04:18.017966Z",
     "iopub.status.busy": "2025-11-30T16:04:18.017400Z",
     "iopub.status.idle": "2025-11-30T16:04:18.038709Z",
     "shell.execute_reply": "2025-11-30T16:04:18.037972Z",
     "shell.execute_reply.started": "2025-11-30T16:04:18.017941Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submission file...\n",
      "        id  winner_model_a  winner_model_b  winner_tie\n",
      "0   136060        0.321311        0.609741    0.068948\n",
      "1   211333        0.341412        0.637842    0.020745\n",
      "2  1233961        0.982696        0.008289    0.009015\n",
      "Saved to submission.csv\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# SUBMISSION\n",
    "# ==========================================\n",
    "\n",
    "print(\"Creating submission file...\")\n",
    "probs_array = np.array(all_probs)\n",
    "\n",
    "# Map indices to columns based on the training class2id:\n",
    "# 0 -> winner_model_a\n",
    "# 1 -> winner_model_b\n",
    "# 2 -> winner_tie\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'winner_model_a': probs_array[:, 0],\n",
    "    'winner_model_b': probs_array[:, 1],\n",
    "    'winner_tie': probs_array[:, 2]\n",
    "})\n",
    "\n",
    "print(submission.head())\n",
    "submission.to_csv(SUBMISSION_PATH, index=False)\n",
    "print(f\"Saved to {SUBMISSION_PATH}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 8346466,
     "isSourceIdPinned": false,
     "sourceId": 66631,
     "sourceType": "competition"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 222398,
     "modelInstanceId": 239467,
     "sourceId": 282742,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": false,
     "modelId": 520100,
     "modelInstanceId": 505198,
     "sourceId": 667311,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
