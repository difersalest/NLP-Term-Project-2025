{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09a14529",
   "metadata": {},
   "source": [
    "# Gemma 3: Fine-Tuning for Classification Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd6924e",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e351c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set the device to physical GPU 3\n",
    "# Physics server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "env_path = \"./config/.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce8d6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = {\"\": 0}\n",
    "gpu_device   = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0b6881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPUs available to PyTorch:\n",
      "----------------------------------------\n",
      "Device Index 0: NVIDIA A100-SXM4-80GB (79.25 GB)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Found {num_gpus} GPUs available to PyTorch:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    name = torch.cuda.get_device_name(i)\n",
    "    mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "    print(f\"Device Index {i}: {name} ({mem:.2f} GB)\")\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f74b68",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af7f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yishin/Didier/human_preference_pred/NLP-Term-Project-2025/Didier_Tests5/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset, load_dataset, DatasetDict \n",
    "\n",
    "from peft import (LoraConfig, \n",
    "                  PeftModel, \n",
    "                  prepare_model_for_kbit_training, \n",
    "                  get_peft_model,\n",
    "                  PeftModelForSequenceClassification,\n",
    "                  PeftConfig)\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForCausalLM)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301b1962",
   "metadata": {},
   "source": [
    "## Logging into hugging face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dff1188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy configured via IP address.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configure the NTHU proxy directly in Python using the IP address\n",
    "proxy_url = \"http://140.114.63.4:3128\"\n",
    "\n",
    "os.environ['http_proxy'] = proxy_url\n",
    "os.environ['https_proxy'] = proxy_url\n",
    "os.environ['HTTP_PROXY'] = proxy_url\n",
    "os.environ['HTTPS_PROXY'] = proxy_url\n",
    "\n",
    "print(\"Proxy configured via IP address.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e93af30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f67a24d",
   "metadata": {},
   "source": [
    "## Loading and preparing the dataset\n",
    "\n",
    "As described in the introduction, we’ll use the thesofakillers/jigsaw-toxic-comment-classification-challenge dataset from the Hugging Face dataset library for this demonstration. We begin by loading only the 'train' portion of this dataset. Since we need distinct sets for training, validation, and testing, we'll perform a couple of splits using the datasets library.\n",
    "\n",
    "The following code executes these steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0835030",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'model_a', 'model_b', 'prompt', 'response_a', 'response_b', 'winner_model_a', 'winner_model_b', 'winner_tie'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_preference = Dataset.from_pandas(pd.read_csv(\"kaggle/input/lmsys-chatbot-arena/train.csv\"))\n",
    "dataset_preference_test = Dataset.from_pandas(pd.read_csv(\"kaggle/input/lmsys-chatbot-arena/test.csv\"))\n",
    "\n",
    "dataset_preference = dataset_preference.train_test_split(test_size=0.20,seed=42,)\n",
    "\n",
    "dataset_preference = DatasetDict({\n",
    "    'train': dataset_preference['train'].select(range(1000)),\n",
    "    'valid': dataset_preference['test'].select(range(100)),\n",
    "    # 'test': dataset_preference_test\n",
    "    })\n",
    "\n",
    "dataset_preference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0bfa0824",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_toxic = load_dataset(\"thesofakillers/jigsaw-toxic-comment-classification-challenge\")\n",
    "# dataset_toxic = dataset_toxic['train']\n",
    "# dataset_toxic = dataset_toxic.train_test_split(test_size=0.25,seed=42,)\n",
    "\n",
    "# test_valid  = dataset_toxic['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "# dataset_toxic = DatasetDict({\n",
    "#     'train': dataset_toxic['train'].select(range(1000)),\n",
    "#     'valid': test_valid['train'].select(range(100)),\n",
    "#     'test': test_valid['test'].select(range(100))})\n",
    "\n",
    "# dataset_toxic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aac60d3",
   "metadata": {},
   "source": [
    "To quickly examine the data’s structure, especially the comments and their labels, we can convert a portion of it, like the training set, into a Pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45daf7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>model_a</th>\n",
       "      <th>model_b</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>winner_model_a</th>\n",
       "      <th>winner_model_b</th>\n",
       "      <th>winner_tie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2941426946</td>\n",
       "      <td>deepseek-llm-67b-chat</td>\n",
       "      <td>mixtral-8x7b-instruct-v0.1</td>\n",
       "      <td>[\"is bird the word? please do tell. thank you ...</td>\n",
       "      <td>[\"I'm sorry, but I'm not sure what you're aski...</td>\n",
       "      <td>[\"I'm glad to help! In terms of the phrase \\\"b...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2474408402</td>\n",
       "      <td>claude-instant-1</td>\n",
       "      <td>tulu-2-dpo-70b</td>\n",
       "      <td>[\"give me the code for a java cmd currency cha...</td>\n",
       "      <td>[\"Here is an example Java program that can act...</td>\n",
       "      <td>[\"Sure! Here's a simple Java command-line curr...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3288410649</td>\n",
       "      <td>claude-2.0</td>\n",
       "      <td>gpt-3.5-turbo-1106</td>\n",
       "      <td>[\"What would have happened if Genghis khan nev...</td>\n",
       "      <td>[\"Here is a summary of how world history may h...</td>\n",
       "      <td>[\"If Genghis Khan had never united the Mongol ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1886605007</td>\n",
       "      <td>solar-10.7b-instruct-v1.0</td>\n",
       "      <td>llama-2-13b-chat</td>\n",
       "      <td>[\"helllo\",\"code python hello world\"]</td>\n",
       "      <td>[\"Hello there! How can I assist you today? Fee...</td>\n",
       "      <td>[\"Hello! How can I assist you today? Is there ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3183848856</td>\n",
       "      <td>gpt-4-0613</td>\n",
       "      <td>gpt-4-1106-preview</td>\n",
       "      <td>[\"please generate a name for a new fictional l...</td>\n",
       "      <td>[\"While your request is noted, it's important ...</td>\n",
       "      <td>[\"I'm sorry, but I cannot fulfill this request...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                    model_a                     model_b  \\\n",
       "0  2941426946      deepseek-llm-67b-chat  mixtral-8x7b-instruct-v0.1   \n",
       "1  2474408402           claude-instant-1              tulu-2-dpo-70b   \n",
       "2  3288410649                 claude-2.0          gpt-3.5-turbo-1106   \n",
       "3  1886605007  solar-10.7b-instruct-v1.0            llama-2-13b-chat   \n",
       "4  3183848856                 gpt-4-0613          gpt-4-1106-preview   \n",
       "\n",
       "                                              prompt  \\\n",
       "0  [\"is bird the word? please do tell. thank you ...   \n",
       "1  [\"give me the code for a java cmd currency cha...   \n",
       "2  [\"What would have happened if Genghis khan nev...   \n",
       "3               [\"helllo\",\"code python hello world\"]   \n",
       "4  [\"please generate a name for a new fictional l...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0  [\"I'm sorry, but I'm not sure what you're aski...   \n",
       "1  [\"Here is an example Java program that can act...   \n",
       "2  [\"Here is a summary of how world history may h...   \n",
       "3  [\"Hello there! How can I assist you today? Fee...   \n",
       "4  [\"While your request is noted, it's important ...   \n",
       "\n",
       "                                          response_b  winner_model_a  \\\n",
       "0  [\"I'm glad to help! In terms of the phrase \\\"b...               0   \n",
       "1  [\"Sure! Here's a simple Java command-line curr...               0   \n",
       "2  [\"If Genghis Khan had never united the Mongol ...               1   \n",
       "3  [\"Hello! How can I assist you today? Is there ...               0   \n",
       "4  [\"I'm sorry, but I cannot fulfill this request...               0   \n",
       "\n",
       "   winner_model_b  winner_tie  \n",
       "0               1           0  \n",
       "1               1           0  \n",
       "2               0           0  \n",
       "3               1           0  \n",
       "4               0           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset_preference['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455521e",
   "metadata": {},
   "source": [
    "This inspection shows the comment_text column alongside the various toxicity labels such as toxic, severe_toxic, obscene etc. These labels are already one-hot encoded. For instance, some comments might be flagged across multiple toxic categories, while many others will display zeros for all labels, indicating they are not hits any category."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2d4046",
   "metadata": {},
   "source": [
    "## Tokenization: Preparing Data for Gemma 3\n",
    "\n",
    "The next stage in our pipeline is tokenization. This process converts the raw text comments from our dataset into a numerical format that the Gemma 3 model can understand and process. For this tutorial, we’re working with the google/gemma-3-4b-it model. The first step is to load its corresponding tokenizer from the Hugging Face Hub. When loading the tokenizer, we'll specify padding_side='right' and add_bos=True to include a beginning-of-sequence token, often beneficial for Gemma models (check the report Table 4).\n",
    "\n",
    "A important part of preparing for a multilabel classification task is creating a clear mapping from our category names to numerical indices. This is achieved with a simple Python dictionary, class2id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76fce763",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_model_id = \"google/gemma-3-1b-it\" # google/gemma-3-4b-it\n",
    "\n",
    "from transformers import AutoTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(hugging_face_model_id,\n",
    "                                          padding_side='right',\n",
    "                                          device_map=device_map,\n",
    "                                          add_bos=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class2id = {'winner_model_a':0,'winner_model_b':1,'winner_tie':2}\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f847298",
   "metadata": {},
   "source": [
    "With the tokenizer ready, we define a preprocess_function. This function will take each sample from our dataset, tokenize its comment_text, and crucially, reformat its multiple binary labels into a single list of 0 or 1 corresponding to our class2id mapping. We'll apply this function across our entire dataset using the .map() method.\n",
    "\n",
    "After tokenization and selecting only the essential columns (input_ids, attention_mask, and our newly created labels), the DatasetDict will look something like this, containing the processed data ready for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "756b8dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess_function(sample):\n",
    "#     labels = []\n",
    "#     for class_ in class2id.keys():\n",
    "#         labels.append(sample[class_])\n",
    "\n",
    "#     sample = tokenizer(f\"\"\"# **Based on the following prompt choose which of the two responses you think humans would prefer the most:** \\n \n",
    "#     ## **Prompt:**\n",
    "#     `{sample['prompt']}`\\n\n",
    "#     ## **Response A:**\n",
    "#     `{sample['response_a']}`\\n\n",
    "#     ## **Response B:**\n",
    "#     `{sample['response_b']}`\"\"\", \n",
    "#                        truncation=True)\n",
    "#     sample['labels'] = labels\n",
    "#     return sample\n",
    "\n",
    "\n",
    "# dataset_preference_tokenized = dataset_preference.map(preprocess_function)\n",
    "# dataset_preference_tokenized = dataset_preference_tokenized.select_columns(['input_ids','attention_mask','labels'])\n",
    "# dataset_preference_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a7b7756",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:03<00:00, 301.28 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 252.66 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Configuration\n",
    "# Set the maximum context length\n",
    "MAX_LENGTH = 8192 # Testing\n",
    "# Buffer for the template text (headers like \"## Prompt:\", special tokens, etc.)\n",
    "TEMPLATE_BUFFER = 200 \n",
    "# Calculate available tokens for actual content\n",
    "AVAILABLE_TOKENS = MAX_LENGTH - TEMPLATE_BUFFER\n",
    "\n",
    "# Define ratios based on the \"Sandwich\" strategy request\n",
    "# 20% for Prompt (Context/Intent), 40% for Resp A, 40% for Resp B (Conclusions)\n",
    "PROMPT_RATIO = 0.2\n",
    "RESP_RATIO = 0.4\n",
    "\n",
    "def preprocess_function(sample):\n",
    "    labels = []\n",
    "    for class_ in class2id.keys():\n",
    "        labels.append(sample[class_])\n",
    "\n",
    "    # Parsing and Concatenation\n",
    "    try:\n",
    "        prompt_text = \"\\n\".join(json.loads(sample['prompt']))\n",
    "        resp_a_text = \"\\n\".join(json.loads(sample['response_a']))\n",
    "        resp_b_text = \"\\n\".join(json.loads(sample['response_b']))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        # Fallback if data is not a valid JSON string\n",
    "        prompt_text = str(sample['prompt'])\n",
    "        resp_a_text = str(sample['response_a'])\n",
    "        resp_b_text = str(sample['response_b'])\n",
    "\n",
    "    # Tokenization for Length Calculation\n",
    "    # We tokenize raw parts to check their lengths against our budget\n",
    "    # add_special_tokens=False because we just want to count content IDs\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)['input_ids']\n",
    "    resp_a_ids = tokenizer(resp_a_text, add_special_tokens=False)['input_ids']\n",
    "    resp_b_ids = tokenizer(resp_b_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # Sandwich / Budget Allocation\n",
    "    \n",
    "    # Calculate max tokens allowed per section\n",
    "    max_prompt_len = int(AVAILABLE_TOKENS * PROMPT_RATIO)\n",
    "    max_resp_len = int(AVAILABLE_TOKENS * RESP_RATIO)\n",
    "\n",
    "    # Truncate Prompt: Keep the START (Head) -> Preserves Intent\n",
    "    if len(prompt_ids) > max_prompt_len:\n",
    "        prompt_ids = prompt_ids[:max_prompt_len]\n",
    "\n",
    "    # Truncate Responses: Keep the END (Tail) -> Preserves Conclusion/Success State\n",
    "    if len(resp_a_ids) > max_resp_len:\n",
    "        resp_a_ids = resp_a_ids[-max_resp_len:] # Slice from end\n",
    "    \n",
    "    if len(resp_b_ids) > max_resp_len:\n",
    "        resp_b_ids = resp_b_ids[-max_resp_len:] # Slice from end\n",
    "\n",
    "    # Decode back to Text\n",
    "    final_prompt = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "    final_resp_a = tokenizer.decode(resp_a_ids, skip_special_tokens=True)\n",
    "    final_resp_b = tokenizer.decode(resp_b_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Final Formatting and Tokenization\n",
    "    formatted_input = f\"\"\"# **Based on the following prompt choose which of the two responses you think humans would prefer the most:** \\\\n \n",
    "    ## **Prompt:**\n",
    "    `{final_prompt}`\\\\n\n",
    "    ## **Response A:**\n",
    "    `{final_resp_a}`\\\\n\n",
    "    ## **Response B:**\n",
    "    `{final_resp_b}`\"\"\"\n",
    "\n",
    "    # Final tokenization with the hard cap to ensure we never exceed physical limits\n",
    "    sample_tokenized = tokenizer(formatted_input, \n",
    "                                 truncation=True, \n",
    "                                 max_length=MAX_LENGTH)\n",
    "    \n",
    "    sample_tokenized['labels'] = labels\n",
    "    return sample_tokenized\n",
    "\n",
    "# Apply the revised function\n",
    "dataset_preference_tokenized = dataset_preference.map(preprocess_function)\n",
    "dataset_preference_tokenized = dataset_preference_tokenized.select_columns(['input_ids','attention_mask','labels'])\n",
    "dataset_preference_tokenized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f9a8fe",
   "metadata": {},
   "source": [
    "To verify our tokenization and label preparation, let’s inspect a single sample from the training set. We’ll look at its raw input_ids and labels, and then decode them back into human-readable tokens and label names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4aefa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data for model:\n",
      "IDs   : [2, 236865, 5213, 22515, 580, 506, 2269, 11172, 5347, 837, 529, 506, 1156, 13630, 611, 1751, 14464, 1093, 5278, 506, 1346, 53121, 621, 236749, 236743, 107, 140, 1408, 5213, 70517, 53121, 107, 140, 236929, 26908, 844, 107, 3970, 23181, 29104, 1902, 236929, 236785, 236749, 107, 140, 1408, 5213, 6126, 562, 53121, 107, 140, 236929, 9259, 993, 236888, 2088, 740, 564, 6361, 611, 3124, 236881, 43213, 2196, 531, 2679, 786, 1027, 4137, 653, 14050, 611, 1093, 1133, 531, 1281, 919, 1003, 236761, 107, 8291, 563, 496, 3606, 17856, 3393, 531, 1887, 623, 9259, 4109, 236775, 580, 506, 3554, 236787, 108, 2717, 6719, 107, 1995, 885, 9259, 4109, 1373, 107, 2717, 108, 2021, 1845, 672, 3393, 236764, 5383, 625, 528, 496, 2129, 607, 506, 123281, 2551, 236929, 9980, 236764, 573, 2591, 2165, 23391, 236779, 12392, 236761, 2551, 21233, 4298, 236764, 1932, 496, 12612, 653, 4991, 1757, 236764, 21332, 531, 506, 12229, 1298, 506, 2129, 563, 10683, 236764, 532, 1845, 506, 2129, 1699, 506, 17856, 56714, 236787, 108, 2717, 107, 6719, 236800, 29104, 236779, 12392, 236761, 2551, 107, 2717, 108, 93446, 236764, 611, 740, 992, 4865, 532, 21836, 506, 3393, 54369, 5467, 1131, 506, 17856, 56714, 568, 1357, 5672, 236768, 653, 1131, 614, 3489, 17856, 56714, 236764, 1288, 618, 59341, 236761, 509, 653, 39626, 85221, 236764, 532, 625, 795, 6842, 13707, 532, 1887, 623, 9259, 4109, 236775, 580, 506, 3554, 82106, 236785, 236749, 107, 140, 1408, 5213, 6126, 603, 53121, 107, 140, 236929, 9259, 236888, 2088, 740, 564, 6361, 611, 3124, 236881, 2375, 993, 2613, 611, 236789, 236753, 1133, 531, 2910, 1003, 653, 2679, 236881, 564, 236789, 236757, 1590, 531, 1601, 607, 1027, 4137, 611, 1149, 735, 236761, 7323, 2514, 528, 3666, 600, 564, 236789, 236757, 1164, 614, 12498, 532, 3914, 2847, 3577, 9106, 653, 15474, 528, 18701, 600, 1149, 577, 27583, 653, 42467, 236761, 107, 49190, 236888, 5715, 563, 614, 2591, 529, 496, 623, 9259, 4109, 236775, 1948, 528, 17856, 236787, 107, 2717, 107, 1995, 885, 9259, 236764, 4109, 26577, 107, 2717, 107, 2094, 1948, 795, 3938, 506, 2483, 623, 9259, 236764, 4109, 9332, 1056, 625, 563, 1845, 236761, 108, 8291, 236789, 236751, 496, 25890, 529, 506, 3393, 236787, 108, 236829, 2165, 1995, 73962, 563, 496, 1292, 600, 4716, 496, 3618, 568, 495, 672, 1624, 236764, 623, 9259, 236764, 4109, 26577, 532, 19365, 625, 580, 506, 4322, 236761, 107, 236829, 2165, 73962, 563, 1456, 531, 2246, 506, 2165, 1995, 73962, 1292, 236761, 108, 2021, 1845, 672, 1948, 236764, 611, 740, 5383, 625, 528, 496, 2129, 607, 496, 123281, 2551, 236929, 9980, 568, 1708, 2591, 236764, 2165, 23391, 236761, 2551, 18833, 532, 1299, 1845, 625, 1699, 506, 17856, 56714, 236761, 2154, 1346, 3797, 236764, 611, 740, 776, 672, 684, 8115, 496, 12612, 653, 4991, 11172, 236764, 67112, 531, 506, 12229, 1298, 611, 10683, 506, 2129, 236764, 532, 28751, 2165, 6719, 29104, 236761, 2551, 21233, 108, 236777, 4614, 672, 7351, 236888, 3792, 786, 1281, 768, 611, 735, 1027, 4137, 82106]\n",
      "Labels: [0, 1, 0]\n",
      "\n",
      "Input data decoded:\n",
      "Tokens: <bos># **Based on the following prompt choose which of the two responses you think humans would prefer the most:** \\n \n",
      "    ## **Prompt:**\n",
      "    `helllo\n",
      "code python hello world`\\n\n",
      "    ## **Response A:**\n",
      "    `Hello there! How can I assist you today? Feel free to ask me any questions or topics you would like to know more about.\n",
      "Here is a simple Python code to print \"Hello World\" on the console:\n",
      "\n",
      "```python\n",
      "print(\"Hello World\")\n",
      "```\n",
      "\n",
      "To run this code, save it in a file with the `.py` extension, for example `hello_world.py`. Then, open a terminal or command line, navigate to the directory where the file is saved, and run the file using the Python interpreter:\n",
      "\n",
      "```\n",
      "python3 hello_world.py\n",
      "```\n",
      "\n",
      "Alternatively, you can also copy and paste the code snippet directly into the Python interpreter (REPL) or into an online Python interpreter, such as Repl.it or Codepen, and it will immediately execute and print \"Hello World\" on the console.`\\n\n",
      "    ## **Response B:**\n",
      "    `Hello! How can I assist you today? Is there something you'd like to talk about or ask? I'm here to help with any questions you may have. Please keep in mind that I'm just an AI and cannot provide personal advice or engage in discussions that may be harmful or inappropriate.\n",
      "Sure! Here is an example of a \"Hello World\" program in Python:\n",
      "```\n",
      "print(\"Hello, World!\")\n",
      "```\n",
      "This program will output the string \"Hello, World!\" when it is run.\n",
      "\n",
      "Here's a breakdown of the code:\n",
      "\n",
      "* `print()` is a function that takes a message (in this case, \"Hello, World!\") and displays it on the screen.\n",
      "* `()` is used to call the `print()` function.\n",
      "\n",
      "To run this program, you can save it in a file with a `.py` extension (for example, `hello.py`) and then run it using the Python interpreter. On most systems, you can do this by opening a terminal or command prompt, navigating to the directory where you saved the file, and typing `python hello.py`.\n",
      "\n",
      "I hope this helps! Let me know if you have any questions.`\n",
      "Label dictionary: {'winner_model_a': 0, 'winner_model_b': 1, 'winner_tie': 0}\n"
     ]
    }
   ],
   "source": [
    "sample_index = 3 # Choose any sample index\n",
    "\n",
    "sample_input_ids = dataset_preference_tokenized['train']['input_ids'][sample_index]\n",
    "sample_labels = dataset_preference_tokenized['train']['labels'][sample_index]\n",
    "\n",
    "print('Input data for model:')\n",
    "print(f\"IDs   : {sample_input_ids}\")\n",
    "print(f\"Labels: {sample_labels}\\n\")\n",
    "\n",
    "print('Input data decoded:')\n",
    "print(f\"Tokens: {tokenizer.decode(sample_input_ids)}\")\n",
    "# Reconstruct the label dictionary for this sample\n",
    "decoded_labels = {id2class[i]: sample_labels[i] for i in range(len(sample_labels))}\n",
    "print(f\"Label dictionary: {decoded_labels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c789722f",
   "metadata": {},
   "source": [
    "This confirms that our text has been converted into a sequence of token IDs, and the corresponding labels are correctly formatted as a list. The decoded output further clarifies how the original sentence and its toxicity classifications are represented, ready for model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b35bcbe",
   "metadata": {},
   "source": [
    "## Dynamic Padding with DataCollator\n",
    "\n",
    "The comments text which we’re using, the length of individual text samples will inevitably vary. While models can process single samples of differing lengths for inference, training is almost always performed in batches to leverage computational efficiency. However, to combine multiple sequences into a single batch for the model, all sequences within that batch must have a uniform length.\n",
    "\n",
    "For our purposes, we’ll use DataCollatorWithPadding from the Hugging Face Transformers library. This utility dynamically pads the shorter sequences in each batch with a special <pad> token until they match the length of the longest sequence in that specific batch.\n",
    "\n",
    "Let’s load the DataCollatorWithPadding and initialize it with our tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72ad58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd83ca46",
   "metadata": {},
   "source": [
    "to observe the data collator in action, we can take a small selection of samples from our tokenized training set — for instance, the first three — and examine their lengths before and after applying the collator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0454d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[258, 1259, 893]\n",
      "[1259, 1259, 1259]\n"
     ]
    }
   ],
   "source": [
    "sample_batch_ids           = dataset_preference_tokenized['train']['input_ids'][0:3]\n",
    "sample_batch_ids_collator  = data_collator(dataset_preference_tokenized['train'][:3])['input_ids'][0:3]\n",
    "print([len(x) for x in sample_batch_ids ])\n",
    "print([len(x) for x in sample_batch_ids_collator ])\n",
    "\n",
    "#length of each sample without datacollator : [74, 37, 159]\n",
    "#length of each sample with datacollator    :[159, 159, 159]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afbc57b4",
   "metadata": {},
   "source": [
    "As the output demonstrates, our initial batch of three samples had varying lengths. After processing with DataCollatorWithPadding, all samples in the input_ids list now share the same length."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fbdba4",
   "metadata": {},
   "source": [
    "## Loading and Adapting the Causal Language Model for Classification\n",
    "\n",
    "With our data tokenized and ready, we now turn to loading the pre-trained model. As discussed earlier, since we’re working under the premise that a direct Gemma3ForSequenceClassification (or similar) class isn't readily available for our hypothetical \"google/gemma-3-4b-it\" model, our strategy is to load it as a causal language model—using a class we'll refer to as Gemma3ForCausalLM—and then tailor it for our multi-label classification task. This adaptation primarily involves replacing its original language modeling head with a new classification head specifically designed for the number of toxic comment categories we aim to predict. The model is then loaded using the from_pretrained method. .\n",
    "\n",
    "Here’s the code to configure quantization and load the base model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "45a0876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(hugging_face_model_id, \n",
    "                                          torch_dtype=torch.bfloat16, \n",
    "                                          device_map=gpu_device,\n",
    "                                          attn_implementation='eager',\n",
    "                                          quantization_config=bnb_config  )\n",
    "\n",
    "model.lm_head = torch.nn.Linear(model.config.hidden_size, len(class2id.keys()), bias=False,device=gpu_device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6d4c61",
   "metadata": {},
   "source": [
    "## Efficient Adaptation with LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Instead of undertaking the computationally intensive process of fine-tuning all parameters of our large Gemma 3 model, we will employ a more efficient and now classical technique: Low-Rank Adaptation, or LoRA. This Parameter-Efficient Fine-Tuning (PEFT) method keeps the vast majority of the pre-trained model weights frozen. The key idea is to inject small, trainable “adapter” layers into specific existing layers of the model. These adapters learn to modify the model’s activations to suit our specific classification task during training, while the original knowledge of the base model remains largely intact. This significantly reduces the number of parameters that need to be updated, leading to faster training and lower memory requirements.\n",
    "\n",
    "Before integrating LoRA, we’ll enable gradient checkpointing on our model, a technique that further reduces memory usage during the backward pass,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bfa5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f36b39",
   "metadata": {},
   "source": [
    "LoRA adapters are typically inserted into the linear layers of a Transformer model. While a helper function like the one below can be used to identify all potential linear layers for LoRA injection (specifically bnb.nn.Linear4bit layers in our 4-bit quantized model, excluding the lm_head), it's also a common and effective practice to use a predefined list of target modules known to yield good performance for a given architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "798585fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "modules = ['gate_proj', 'down_proj', 'v_proj', 'k_proj', 'q_proj', 'o_proj', 'up_proj']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11266fd5",
   "metadata": {},
   "source": [
    "With our target modules identified (or specified), we define the LoraConfig. This configuration specifies parameters like the rank (r) of the adapter matrices, the LoRA alpha (lora_alpha) scaling factor, the target_modules we just defined, a dropout rate (lora_dropout) for regularization, and importantly, the task_type which is set to \"SEQ_CLS\" to align with our sequence classification objective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09a01a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af25f2a1",
   "metadata": {},
   "source": [
    "After applying this configuration using get_peft_model, the model object is now enhanced with LoRA adapters. A call to model.print_trainable_parameters() will clearly demonstrate the efficiency of this approach, showing that only a small fraction of the total parameters are now trainable, drastically reducing the fine-tuning burden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1c6edf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 52,183,040 || all params: 1,052,072,448 || trainable%: 4.9600\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n",
    "#trainable params: 119,209,984 || all params: 3,999,488,512 || trainable%: 2.9806"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d596cf",
   "metadata": {},
   "source": [
    "## Customizing PEFT for Sequence Classification with a Causal LM\n",
    "\n",
    "This section introduces a key customization that enables us to effectively train our LoRA-adapted causal Gemma 3 model for sequence classification. While PEFT provides PeftModelForSequenceClassification, we'll create a slightly tailored version. This custom class, which we'll call Gemma3ForSequenceClassification, ensures that the forward pass and loss calculation are correctly handled for our specific setup, which uses a base causal model modified for multi-label classification.\n",
    "\n",
    "The core of this adaptation lies in how we process the model’s outputs. A causal language model, even with a replaced head, produces logits for every token in the sequence. For sequence classification, we are typically interested in a single representation for the entire sequence. A common strategy, adopted here, is to use the logits corresponding to the last token of the sequence as the input to our classification loss function. Our custom class implements this logic and incorporates the appropriate loss function for multi-label tasks, BCEWithLogitsLoss.\n",
    "\n",
    "Here’s the definition of our Gemma3ForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1ca21f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma3ForSequenceClassification(PeftModelForSequenceClassification):\n",
    "    def __init__(self, peft_config: PeftConfig, model: AutoModelForCausalLM, adapter_name=\"default\"):\n",
    "        super().__init__(model, peft_config, adapter_name)\n",
    "        self.num_labels = model.config.num_labels\n",
    "        self.problem_type = \"multi_label_classification\" # Assuming multi-label\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.return_dict\n",
    "\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "\n",
    "        # Extract logits from the outputs\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # select last \"real\" token and ignore padding tokens\n",
    "\n",
    "        sequence_lengths   = torch.sum(attention_mask, dim=1)\n",
    "        last_token_indices = sequence_lengths - 1\n",
    "        batch_size         = logits.shape[0]\n",
    "       \n",
    "        # Get the logits for the last token in the sequence\n",
    "        logits = logits[torch.arange(batch_size, device=logits.device), last_token_indices, :]\n",
    "        #logits = logits[:, -1, :] # if batch_size = 1\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.problem_type == \"regression\":\n",
    "                loss_fct = torch.nn.MSELoss()\n",
    "                loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "            elif self.problem_type == \"single_label_classification\":\n",
    "                loss_fct = torch.nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e646e",
   "metadata": {},
   "source": [
    "With this custom class defined, we then instantiate it. We create a PeftConfig object, populating it with the parameters from our lora_config (defined in the previous LoRA setup step). This ensures that our Gemma3ForSequenceClassification wrapper is aware of the LoRA configuration. The model argument passed here is the LoRA-enhanced model we obtained from get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "21fb58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PeftConfig(peft_type=\"LORA\", task_type=\"SEQ_CLS\", inference_mode=False)\n",
    "for key, value in lora_config.__dict__.items():\n",
    "    setattr(peft_config, key, value)\n",
    "\n",
    "wrapped_model = Gemma3ForSequenceClassification(peft_config, model)\n",
    "wrapped_model.num_labels = len(class2id.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d623a2d",
   "metadata": {},
   "source": [
    "## Implementing a Custom Loss Function (Optional)\n",
    "\n",
    "While the Hugging Face Trainer and PyTorch provide robust, standard loss functions—and for multi-label classification, torch.nn.BCEWithLogitsLoss is generally the recommended, numerically stable choice—there might be scenarios where you wish to define or understand the loss calculation more explicitly, or perhaps introduce unique modifications. This section demonstrates how to implement a binary cross-entropy loss manually and then integrate it by creating a custom Trainer class. It's important to note that directly implementing sigmoid followed by log calculations can be less numerically stable than combined functions like BCEWithLogitsLoss, but this approach offers a clear view of the underlying mechanics.\n",
    "\n",
    "First, let’s define our custom binary cross-entropy function. This function will take raw logits and true labels as input. It applies a sigmoid function to the logits to obtain probabilities, clamps these probabilities to a small range (epsilon to 1-epsilon) to avoid log(0) issues for numerical stability, and then calculates the binary cross-entropy loss,\n",
    "\n",
    "first all define our the function for the binary crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d947dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_binary_crossentropy_loss(logits, labels,epsilon=1e-7):\n",
    "  \n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs = torch.clamp(probs, min=epsilon, max=1-epsilon) # capping values\n",
    "    loss  = -(labels * torch.log(probs) + (1 - labels) * torch.log(1 - probs))\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7277a5f4",
   "metadata": {},
   "source": [
    "To utilize this custom_binary_crossentropy_loss function within the standard Hugging Face training workflow, we can create a new class, CustomTrainer, that inherits from the base Trainer class. We then override its compute_loss method to incorporate our custom calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "09d53b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):     \n",
    "    def compute_loss(self, model, inputs,num_items_in_batch=4, return_outputs=False): \n",
    "        labels  = inputs.get(\"labels\")\n",
    "        inputs  = inputs.to(gpu_device)\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.logits \n",
    "        \n",
    "        loss    = custom_binary_crossentropy_loss(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72461fe",
   "metadata": {},
   "source": [
    "In this CustomTrainer, the compute_loss method is overridden. It first prepares the inputs and retrieves the true labels. The model then performs a forward pass with the remaining inputs to produce outputs, from which we extract the logits. Our custom_binary_crossentropy_loss function is then called with these logits and the labels to calculate the loss. The method returns the loss, and optionally the model's outputs, aligning with the expected behavior of the Trainer's compute_loss method.\n",
    "\n",
    "This customized Trainer can now be used in place of the standard Trainer if you wish to proceed with this explicit loss computation method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73925efd",
   "metadata": {},
   "source": [
    "## Defining Evaluation Metrics\n",
    "\n",
    "To measure our multi-label classifier’s performance, we’ll use standard metrics: accuracy, F1-score, precision, and recall. The Hugging Face evaluate library conveniently groups these for us. Since our model outputs raw logits, we'll first need a sigmoid function to convert these into probabilities (0 to 1).\n",
    "\n",
    "The compute_metrics function, designed for the Hugging Face Trainer, handles the main logic. It takes the model's predicted logits and true labels. Inside, it applies the sigmoid function to the logits, then converts these probabilities into binary (0 or 1) predictions using a 0.5 threshold. For multi-label evaluation with these metrics, both the binarized predictions and the true labels are flattened. This approach treats each label for each sample as an independent prediction, allowing us to calculate an overall performance score. These processed arrays are then fed into our combined metric evaluator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f300ed46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ef7620",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "\n",
    "With our model, data, and evaluation components prepared, we’re ready to begin the training process. First, we’ll set up an EarlyStoppingCallback to monitor performance and halt training if the model stops improving, preventing overfitting. We also specify a directory to save our model checkpoints. Additionally, it's often useful to manage tokenizer parallelism by setting an environment variable, which can prevent potential issues with some multi-processing setups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "24326ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback, TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "# Define early stopping and checkpoint directory\n",
    "early_stop = EarlyStoppingCallback(early_stopping_patience=3, # Increased patience slightly\n",
    "                                   early_stopping_threshold=0.001) # A small threshold\n",
    "checkpoints_dir = 'preference_class_gemma_model' # More descriptive name\n",
    "\n",
    "# Set tokenizer parallelism environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fb535c",
   "metadata": {},
   "source": [
    "Next, we configure TrainingArguments. This object holds a wide array of hyperparameters and settings that control the training loop. Key settings include the output directory for checkpoints, learning rate, batch sizes for training and evaluation, the number of epochs, weight decay for regularization, evaluation and saving strategies, and the metric for identifying the best model (here, eval_loss). We are also enabling mixed-precision training (fp16=True) for efficiency and setting up gradient accumulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd590da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    gradient_checkpointing=False,  # Gradient Checkpointing ist nicht aktiviert\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    #label_names=classes,\n",
    "    dataloader_num_workers=4,\n",
    "    output_dir= checkpoints_dir ,  # Output directory for checkpoints\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=8,  # Batch size per device\n",
    "    per_device_eval_batch_size=8,  # Batch size per device for evaluation \n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    eval_strategy='epoch',  # Evaluate after each epoch\n",
    "    #eval_steps=100,\n",
    "    save_strategy=\"epoch\",  # Save model checkpoints after each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model based on the chosen metric\n",
    "    push_to_hub=False,  # Disable pushing the model to the Hugging Face Hub \n",
    "    report_to=\"tensorboard\",  # Disable logging to Weight&Bias\n",
    "    logging_dir =  f\"tensorboard_my_model\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    "    warmup_ratio =0.05, \n",
    "    metric_for_best_model='eval_loss',)  # Metric for selecting the best model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18a665d",
   "metadata": {},
   "source": [
    "With the arguments defined, we instantiate the Trainer. This brings together our LoRA-adapted wrapped_model, the training_args, the tokenized training and validation datasets, our data_collator for creating batches, the compute_metrics function, and the early_stop callback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cc898b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer (\n",
    "    model=wrapped_model,  # The LoRA-adapted model\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=dataset_preference_tokenized['train'],  # Training dataset\n",
    "    eval_dataset=dataset_preference_tokenized['valid'],  # Evaluation dataset\n",
    "    #tokenizer=tokenizer,  # Tokenizer for processing text\n",
    "    data_collator=data_collator,  # Data collator for preparing batches\n",
    "    compute_metrics=compute_metrics,  # Function to calculate evaluation metrics\n",
    "    callbacks=[early_stop]  # Optional early stopping callback\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e03c3a1",
   "metadata": {},
   "source": [
    "Finally, calling trainer.train() launches the fine-tuning process. The trainer will handle the training loop, evaluation, and checkpointing according to our defined arguments.\n",
    "This will begin training your custom Gemma 3 classification model. Monitor the logs and evaluation metrics to observe its learning progress.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "aa3decba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/yishin/Didier/human_preference_pred/NLP-Term-Project-2025/Didier_Tests5/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 12:34, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.654957</td>\n",
       "      <td>0.636667</td>\n",
       "      <td>0.167939</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.110000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.652347</td>\n",
       "      <td>0.626667</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.210000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.644782</td>\n",
       "      <td>0.660000</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.480769</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yishin/Didier/human_preference_pred/NLP-Term-Project-2025/Didier_Tests5/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/yishin/Didier/human_preference_pred/NLP-Term-Project-2025/Didier_Tests5/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=2.5194732348124185, metrics={'train_runtime': 760.1775, 'train_samples_per_second': 3.946, 'train_steps_per_second': 0.126, 'total_flos': 2.463351535890432e+16, 'train_loss': 2.5194732348124185, 'epoch': 3.0})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce2cf9d",
   "metadata": {},
   "source": [
    "For the purpose of this demonstration, the dataset was downsampled to a smaller subset of samples. The training process on this reduced dataset successfully showed a clear reduction in the second epoch, alongside corresponding improvements in evaluation metrics such as accuracy and F1-score, indicating that the model was learning effectively from the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefd537d",
   "metadata": {},
   "source": [
    "## Making Predictions with the Fine-Tuned Model\n",
    "\n",
    "Now that our model has been trained, the next step is to use it to make predictions on new, unseen data, such as the comments in our test set. To facilitate this, we’ll first define a helper function, prediction. This function will take a raw text string as input, process it through our trained model, and then return the predicted probabilities for each toxicity category, with the labels sorted by the model's confidence. We'll need our tokenizer, the trained wrapped_model, the id2class mapping (from label indices back to names), and the target device (e.g., \"cuda:0\") to be available from our previous setup.\n",
    "\n",
    "The prediction function tokenizes the input text and sends it to the specified device. It then performs inference using our wrapped_model within a torch.no_grad() context to disable gradient calculations, which are unnecessary for prediction and save memory. After obtaining the logits, it applies a sigmoid function to convert them into probabilities. These probabilities are then paired with their corresponding class labels (via id2class), and both are sorted in descending order of probability to clearly show the model's most confident predictions first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c20e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prediction(input_text):\n",
    "#     inputs          = tokenizer(input_text, return_tensors=\"pt\",).to(\"cuda:0\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = wrapped_model(**inputs).logits\n",
    "#     y_prob          = np.round(np.array(torch.sigmoid(outputs).tolist()[0]),5)\n",
    "#     y_sorted_labels = [id2class.get(y) for y  in np.argsort(y_prob)[::-1]]\n",
    "#     y_prob_sorted   = np.sort(y_prob)[::-1]\n",
    "    \n",
    "#     return y_sorted_labels,y_prob_sorted  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8032c15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Configuration (Must match training)\n",
    "# MAX_LENGTH = 8192 \n",
    "# TEMPLATE_BUFFER = 200 \n",
    "# AVAILABLE_TOKENS = MAX_LENGTH - TEMPLATE_BUFFER\n",
    "# PROMPT_RATIO = 0.2\n",
    "# RESP_RATIO = 0.4\n",
    "\n",
    "# Helper to Format Input (Sandwich Strategy)\n",
    "def prepare_inference_input(row):\n",
    "    # Parse JSON strings to actual text\n",
    "    try:\n",
    "        prompt_text = \"\\n\".join(json.loads(row['prompt']))\n",
    "        resp_a_text = \"\\n\".join(json.loads(row['response_a']))\n",
    "        resp_b_text = \"\\n\".join(json.loads(row['response_b']))\n",
    "    except (json.JSONDecodeError, TypeError):\n",
    "        prompt_text = str(row['prompt'])\n",
    "        resp_a_text = str(row['response_a'])\n",
    "        resp_b_text = str(row['response_b'])\n",
    "\n",
    "    # Tokenize to check lengths\n",
    "    prompt_ids = tokenizer(prompt_text, add_special_tokens=False)['input_ids']\n",
    "    resp_a_ids = tokenizer(resp_a_text, add_special_tokens=False)['input_ids']\n",
    "    resp_b_ids = tokenizer(resp_b_text, add_special_tokens=False)['input_ids']\n",
    "\n",
    "    # Apply Budget (Sandwich Logic)\n",
    "    max_prompt_len = int(AVAILABLE_TOKENS * PROMPT_RATIO)\n",
    "    max_resp_len = int(AVAILABLE_TOKENS * RESP_RATIO)\n",
    "\n",
    "    # Prompt: Keep Start\n",
    "    if len(prompt_ids) > max_prompt_len:\n",
    "        prompt_ids = prompt_ids[:max_prompt_len]\n",
    "\n",
    "    # Responses: Keep End\n",
    "    if len(resp_a_ids) > max_resp_len:\n",
    "        resp_a_ids = resp_a_ids[-max_resp_len:] \n",
    "    \n",
    "    if len(resp_b_ids) > max_resp_len:\n",
    "        resp_b_ids = resp_b_ids[-max_resp_len:] \n",
    "\n",
    "    # Decode back to text\n",
    "    final_prompt = tokenizer.decode(prompt_ids, skip_special_tokens=True)\n",
    "    final_resp_a = tokenizer.decode(resp_a_ids, skip_special_tokens=True)\n",
    "    final_resp_b = tokenizer.decode(resp_b_ids, skip_special_tokens=True)\n",
    "\n",
    "    # Construct the Final Formatted String\n",
    "    return f\"\"\"# **Based on the following prompt choose which of the two responses you think humans would prefer the most:** \\\\n \n",
    "    ## **Prompt:**\n",
    "    `{final_prompt}`\\\\n\n",
    "    ## **Response A:**\n",
    "    `{final_resp_a}`\\\\n\n",
    "    ## **Response B:**\n",
    "    `{final_resp_b}`\"\"\"\n",
    "\n",
    "# The Prediction Function\n",
    "def prediction(formatted_text):\n",
    "    # Ensure inputs don't exceed model limits even after manual formatting\n",
    "    inputs = tokenizer(formatted_text, \n",
    "                       return_tensors=\"pt\", \n",
    "                       truncation=True, \n",
    "                       max_length=MAX_LENGTH).to(\"cuda:0\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Use your wrapped_model (or loaded PEFT model)\n",
    "        outputs = wrapped_model(**inputs).logits\n",
    "        \n",
    "    y_prob = np.round(np.array(torch.sigmoid(outputs).cpu().tolist()[0]), 5)\n",
    "    \n",
    "    # Sort labels by probability\n",
    "    y_sorted_indices = np.argsort(y_prob)[::-1]\n",
    "    y_sorted_labels = [id2class.get(y) for y in y_sorted_indices]\n",
    "    y_prob_sorted = y_prob[y_sorted_indices]\n",
    "    \n",
    "    return y_sorted_labels, y_prob_sorted"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a169ef",
   "metadata": {},
   "source": [
    "With this function defined, we can now apply it to our test dataset. We’ll convert the test portion of dataset_toxic into a Pandas DataFrame for convenient processing. Then, using the .map() method, we'll apply our prediction function to each comment_text. The returned sorted labels and probabilities will be stored in new columns in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8d127dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_test = pd.read_csv(\"kaggle/input/lmsys-chatbot-arena/test.csv\")\n",
    "\n",
    "# df_test['pred'] = df_test['comment_text'].map(prediction)\n",
    "# df_test['argsort_label']  = df_test['pred'].apply(lambda x : x[0])\n",
    "# df_test['argsort_prob']   = df_test['pred'].apply(lambda x : x[1])\n",
    "# print(df_test.shape)\n",
    "# df_test.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84692f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting inputs (Sandwich Strategy)...\n",
      "Running predictions...\n",
      "(3, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>argsort_label</th>\n",
       "      <th>argsort_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[winner_tie, winner_model_a, winner_model_b]</td>\n",
       "      <td>[0.6259, 0.22001, 0.20546]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[winner_model_a, winner_model_b, winner_tie]</td>\n",
       "      <td>[0.41876, 0.38439, 0.20498]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                 argsort_label  \\\n",
       "0  136060  [winner_tie, winner_model_a, winner_model_b]   \n",
       "1  211333  [winner_model_a, winner_model_b, winner_tie]   \n",
       "\n",
       "                  argsort_prob  \n",
       "0   [0.6259, 0.22001, 0.20546]  \n",
       "1  [0.41876, 0.38439, 0.20498]  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Execution on DataFrame\n",
    "\n",
    "# Load Test Data\n",
    "df_test = pd.read_csv(\"kaggle/input/lmsys-chatbot-arena/test.csv\")\n",
    "\n",
    "print(\"Formatting inputs (Sandwich Strategy)...\")\n",
    "# Apply the formatting to every row (axis=1)\n",
    "df_test['formatted_input'] = df_test.apply(prepare_inference_input, axis=1)\n",
    "\n",
    "print(\"Running predictions...\")\n",
    "# Run prediction on the formatted text\n",
    "df_test['pred'] = df_test['formatted_input'].map(prediction)\n",
    "\n",
    "# Extract results\n",
    "df_test['argsort_label'] = df_test['pred'].apply(lambda x: x[0])\n",
    "df_test['argsort_prob'] = df_test['pred'].apply(lambda x: x[1])\n",
    "\n",
    "# Clean up temporary columns\n",
    "# df_test = df_test.drop(columns=['formatted_input', 'pred'])\n",
    "\n",
    "print(df_test.shape)\n",
    "df_test[['id', 'argsort_label', 'argsort_prob']].head(n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b0f3e5",
   "metadata": {},
   "source": [
    "Inspecting the first few rows of df_test (as shown in the example output table \"Result of the dataframe\" with \"first two samples...\") allows us to see the model's predictions directly alongside the original comments. For instance, looking at the second sample in such an output, we might observe that the 'toxic' category has a high probability (e.g., 0.95), while 'insult' and 'obscene' also show significant probabilities (e.g., around 0.8), and other categories have lower scores. This gives a direct insight into the model's assessment for each comment.\n",
    "\n",
    "This demonstrates a straightforward method to obtain and examine predictions. While various techniques exist for applying thresholds to these probabilities to derive final binary decisions for each label, this guide focuses on showcasing the raw predictive output for the test data, rather than delving into those specific post-processing strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3762b",
   "metadata": {},
   "source": [
    "## Saving the Fine-Tuned Model and LoRA Adapters\n",
    "\n",
    "Saving your fine-tuned model and its LoRA adapters correctly is crucial for future use, deployment, or sharing. The standard approach involves using the save_pretrained method available on your trained model object. This will save the adapter weights and an adapter configuration file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "41f10405",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir =  f'preference_class_gemma_1'\n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "797ffa31",
   "metadata": {},
   "source": [
    "While save_pretrained handles saving the adapter weights (usually in adapter_model.bin or .safetensors) and creates an adapter_config.json, special attention is needed for this configuration file. Given that we've made custom adjustments, particularly by potentially using a custom PEFT class wrapper like our Gemma3ForSequenceClassification, the automatically generated adapter_config.json might sometimes lack certain specific LoRA parameters or may not fully capture the nuances of our setup. An incomplete or misconfigured adapter_config.json can lead to difficulties when you later try to load the model with the PEFT library.\n",
    "\n",
    "To ensure this configuration file is robust and accurately reflects your setup, it’s prudent to programmatically verify and augment it. The following script demonstrates how you can load the saved adapter_config.json, compare it against your original LoraConfig object (which we'll refer to as lora_config from your setup) and the base model's Hugging Face ID, and then update any missing or incorrect fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "21a5fa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually updated adapter configuration: preference_class_gemma_1/adapter_config.json\n",
      "New content: {'auto_mapping': None, 'base_model_name_or_path': 'google/gemma-3-1b-it', 'inference_mode': True, 'peft_type': 'LORA', 'revision': None, 'task_type': 'SEQ_CLS', 'r': 64, 'lora_alpha': 32, 'lora_dropout': 0.1, 'target_modules': ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj'], 'bias': 'none', 'modules_to_save': ['classifier', 'score', 'classifier', 'score'], 'fan_in_fan_out': False, 'init_lora_weights': True}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "# Assuming lora_config is the LoraConfig object used during setup\n",
    "# Assuming hugging_face_model_id is the string ID like \"google/gemma-3-4b-it\"\n",
    "# Assuming output_dir is the path where the model was saved\n",
    "\n",
    "adapter_config_path = os.path.join(output_dir, \"adapter_config.json\")\n",
    "\n",
    "# Check if file exists before proceeding\n",
    "if os.path.exists(adapter_config_path):\n",
    "    try:\n",
    "        # Load the potentially incomplete config\n",
    "        with open(adapter_config_path, 'r') as f:\n",
    "            saved_config_dict = json.load(f)\n",
    "\n",
    "        # Get parameters from the original LoraConfig\n",
    "        # Use.to_dict() if available, otherwise __dict__\n",
    "        try:\n",
    "            # Ensure lora_config is the actual LoraConfig object instance\n",
    "            lora_config_dict = lora_config.to_dict()\n",
    "        except AttributeError:\n",
    "            # Fallback, might include extra internal attributes\n",
    "            lora_config_dict = lora_config.__dict__\n",
    "            # Clean up potential internal attributes if using __dict__\n",
    "            lora_config_dict = {k: v for k, v in lora_config_dict.items() if not k.startswith('_')}\n",
    "\n",
    "\n",
    "        # *** FIX 1: Define the specific keys to check ***\n",
    "        # These are common LoRA parameters that might be missing\n",
    "        lora_keys_to_check = [\n",
    "            \"r\",\n",
    "            \"lora_alpha\",\n",
    "            \"lora_dropout\",\n",
    "            \"target_modules\",\n",
    "            \"bias\",\n",
    "            \"modules_to_save\", # Important if you used it\n",
    "            \"fan_in_fan_out\",\n",
    "            \"init_lora_weights\",\n",
    "            # Add any other specific keys from your LoraConfig if needed\n",
    "        ]\n",
    "\n",
    "        # Merge missing or None parameters from the original lora_config\n",
    "        updated = False\n",
    "        for key in lora_keys_to_check:\n",
    "            # Check if key is missing in saved config OR if it exists but is None\n",
    "            if key not in saved_config_dict or saved_config_dict[key] is None:\n",
    "                # Check if the key exists in the original config and has a value\n",
    "                if key in lora_config_dict and lora_config_dict[key] is not None:\n",
    "                    saved_config_dict[key] = lora_config_dict[key]\n",
    "                    updated = True\n",
    "\n",
    "        # Ensure essential base fields are present and correct\n",
    "        # Use getattr for safer access to lora_config attributes\n",
    "        original_task_type = getattr(lora_config, 'task_type', 'SEQ_CLS')\n",
    "        if 'task_type' not in saved_config_dict or saved_config_dict['task_type']!= original_task_type:\n",
    "             saved_config_dict['task_type'] = original_task_type\n",
    "             updated = True\n",
    "\n",
    "        original_base_model = getattr(lora_config, 'base_model_name_or_path', hugging_face_model_id)\n",
    "        if 'base_model_name_or_path' not in saved_config_dict or saved_config_dict['base_model_name_or_path']!= original_base_model:\n",
    "             saved_config_dict['base_model_name_or_path'] = original_base_model\n",
    "             updated = True\n",
    "\n",
    "        if 'peft_type' not in saved_config_dict or saved_config_dict['peft_type']!= \"LORA\":\n",
    "             saved_config_dict['peft_type'] = \"LORA\"\n",
    "             updated = True\n",
    "\n",
    "        # *** FIX 2: Convert set to list before saving ***\n",
    "        if 'target_modules' in saved_config_dict and isinstance(saved_config_dict['target_modules'], set):\n",
    "            saved_config_dict['target_modules'] = sorted(list(saved_config_dict['target_modules'])) # Convert set to sorted list\n",
    "            updated = True # Mark as updated if conversion happened\n",
    "\n",
    "        if 'modules_to_save' in saved_config_dict and isinstance(saved_config_dict['modules_to_save'], set):\n",
    "             # Also handle modules_to_save if it could be a set\n",
    "             saved_config_dict['modules_to_save'] = sorted(list(saved_config_dict['modules_to_save']))\n",
    "             updated = True\n",
    "\n",
    "\n",
    "        # Overwrite the config file only if changes were made\n",
    "        if updated:\n",
    "            with open(adapter_config_path, 'w') as f:\n",
    "                # Save the corrected dictionary as JSON\n",
    "                json.dump(saved_config_dict, f, indent=2)\n",
    "            print(f\"Manually updated adapter configuration: {adapter_config_path}\")\n",
    "            print(\"New content:\", saved_config_dict) # Optional: print the final dict\n",
    "        else:\n",
    "            print(f\"Adapter configuration already seemed complete or no changes needed: {adapter_config_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during manual update of adapter_config.json: {e}\")\n",
    "else:\n",
    "    print(f\"Error: adapter_config.json not found at {adapter_config_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084f0e4d",
   "metadata": {},
   "source": [
    "After running this script, if any modifications were needed, your adapter_config.json will be updated. This manual verification step helps ensure that all relevant details of your LoRA setup are accurately stored, which is key for reliably loading and reusing your fine-tuned adapter model with PEFT at a later stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd675ecd",
   "metadata": {},
   "source": [
    "## Reloading the Saved Model and Making Predictions\n",
    "\n",
    "To use your fine-tuned model later, you’ll need to reload it. This involves setting up the base model architecture again, including any custom classes like our Gemma3ForSequenceClassification, and then loading the saved LoRA adapter weights. Ensure all necessary libraries, your custom class definitions, and configurations like class2id are available in your environment.\n",
    "\n",
    "First, we re-initialize the tokenizer and the base Gemma3ForCausalLM model with the same quantization settings (bnb_config) used during training. The model's language modeling head is then replaced with a new linear layer matching the number of labels for our classification task. We also re-establish the LoRA configuration that defines how adapters are applied. The base model, now with its classification head, is then wrapped using our Gemma3ForSequenceClassification class. This prepared structure is crucial for correctly loading and interpreting the saved adapter weights.\n",
    "\n",
    "With this setup in place, PeftModel.from_pretrained is used to load the trained LoRA adapter weights from your specified output_dir into the Gemma3ForSequenceClassification instance. We set is_trainable=False and switch the model to evaluation mode with model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "360e9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir =  'preference_class_gemma_1'\n",
    "hugging_face_model_id = \"google/gemma-3-1b-it\" # gemma-3-4b-it\n",
    "gpu_device = 'cuda:0'\n",
    "\n",
    "from transformers import AutoTokenizer \n",
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hugging_face_model_id,\n",
    "                                          padding_side='right',\n",
    "                                          device_map=gpu_device,\n",
    "                                          add_bos=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class2id = {'winner_model_a':0,'winner_model_b':1,'winner_tie':2}\n",
    "id2class = {v: k for k, v in class2id.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "base_model = Gemma3ForCausalLM.from_pretrained(hugging_face_model_id, \n",
    "                                          torch_dtype=torch.bfloat16, \n",
    "                                          device_map=gpu_device,\n",
    "                                          attn_implementation='eager',\n",
    "                                          quantization_config=bnb_config  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "160accc1",
   "metadata": {},
   "source": [
    "and Lora adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1cecd123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing lm_head for 3 classes.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "modules = ['gate_proj', 'down_proj', 'v_proj', 'k_proj', 'q_proj', 'o_proj', 'up_proj']\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",modules_to_save=['lm_head'])\n",
    "\n",
    "peft_config = PeftConfig(peft_type=\"LORA\", task_type=\"SEQ_CLS\", inference_mode=False)\n",
    "for key, value in lora_config.__dict__.items():\n",
    "    setattr(peft_config, key, value)\n",
    "\n",
    "\n",
    "num_labels = len(id2class.keys()) # Must match the number of classes used during training\n",
    "load_dtype = torch.bfloat16 # Match training or desired inference precision\n",
    "print(f\"Replacing lm_head for {num_labels} classes.\")\n",
    "base_model.lm_head = torch.nn.Linear(\n",
    "    base_model.config.hidden_size,\n",
    "    num_labels,\n",
    "    bias=False,\n",
    "    device=base_model.device # Ensure head is on the correct device\n",
    ").to(dtype=load_dtype) # Ensure head matches model dtype\n",
    "\n",
    "\n",
    "base_model = Gemma3ForSequenceClassification(peft_config, base_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8238496d",
   "metadata": {},
   "source": [
    "load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3cadcffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForSequenceClassification(\n",
       "      (base_model): LoraModel(\n",
       "        (model): Gemma3ForCausalLM(\n",
       "          (model): Gemma3TextModel(\n",
       "            (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "            (layers): ModuleList(\n",
       "              (0-25): 26 x Gemma3DecoderLayer(\n",
       "                (self_attn): Gemma3Attention(\n",
       "                  (q_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1152, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                )\n",
       "                (mlp): Gemma3MLP(\n",
       "                  (gate_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=6912, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (up_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=6912, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (down_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=6912, out_features=1152, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=6912, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (act_fn): GELUTanh()\n",
       "                )\n",
       "                (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "                (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "                (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "                (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "              )\n",
       "            )\n",
       "            (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (rotary_emb): Gemma3RotaryEmbedding()\n",
       "            (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "          )\n",
       "          (lm_head): ModulesToSaveWrapper(\n",
       "            (original_module): Linear(in_features=1152, out_features=3, bias=False)\n",
       "            (modules_to_save): ModuleDict(\n",
       "              (default): Linear(in_features=1152, out_features=3, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    output_dir,\n",
    "    is_trainable=False # Set to False for inference\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36842716",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def prediction(input_text):\n",
    "#     inputs          = tokenizer(input_text, return_tensors=\"pt\",).to(\"cuda:0\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(**inputs).logits\n",
    "#     y_prob          = np.round(np.array(torch.sigmoid(outputs).tolist()[0]),5)\n",
    "#     y_sorted_labels = [id2class.get(y) for y  in np.argsort(y_prob)[::-1]]\n",
    "#     y_prob_sorted   = np.sort(y_prob)[::-1]\n",
    "    \n",
    "#     return y_sorted_labels,y_prob_sorted "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37563d5",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d0f6949e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt</th>\n",
       "      <th>response_a</th>\n",
       "      <th>response_b</th>\n",
       "      <th>formatted_input</th>\n",
       "      <th>pred</th>\n",
       "      <th>argsort_label</th>\n",
       "      <th>argsort_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>136060</td>\n",
       "      <td>[\"I have three oranges today, I ate an orange ...</td>\n",
       "      <td>[\"You have two oranges today.\"]</td>\n",
       "      <td>[\"You still have three oranges. Eating an oran...</td>\n",
       "      <td># **Based on the following prompt choose which...</td>\n",
       "      <td>([winner_tie, winner_model_a, winner_model_b],...</td>\n",
       "      <td>[winner_tie, winner_model_a, winner_model_b]</td>\n",
       "      <td>[0.6259, 0.22001, 0.20546]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>211333</td>\n",
       "      <td>[\"You are a mediator in a heated political deb...</td>\n",
       "      <td>[\"Thank you for sharing the details of the sit...</td>\n",
       "      <td>[\"Mr Reddy and Ms Blue both have valid points ...</td>\n",
       "      <td># **Based on the following prompt choose which...</td>\n",
       "      <td>([winner_model_a, winner_model_b, winner_tie],...</td>\n",
       "      <td>[winner_model_a, winner_model_b, winner_tie]</td>\n",
       "      <td>[0.41876, 0.38439, 0.20498]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1233961</td>\n",
       "      <td>[\"How to initialize the classification head wh...</td>\n",
       "      <td>[\"When you want to initialize the classificati...</td>\n",
       "      <td>[\"To initialize the classification head when p...</td>\n",
       "      <td># **Based on the following prompt choose which...</td>\n",
       "      <td>([winner_tie, winner_model_a, winner_model_b],...</td>\n",
       "      <td>[winner_tie, winner_model_a, winner_model_b]</td>\n",
       "      <td>[0.55393, 0.35824, 0.14187]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                             prompt  \\\n",
       "0   136060  [\"I have three oranges today, I ate an orange ...   \n",
       "1   211333  [\"You are a mediator in a heated political deb...   \n",
       "2  1233961  [\"How to initialize the classification head wh...   \n",
       "\n",
       "                                          response_a  \\\n",
       "0                    [\"You have two oranges today.\"]   \n",
       "1  [\"Thank you for sharing the details of the sit...   \n",
       "2  [\"When you want to initialize the classificati...   \n",
       "\n",
       "                                          response_b  \\\n",
       "0  [\"You still have three oranges. Eating an oran...   \n",
       "1  [\"Mr Reddy and Ms Blue both have valid points ...   \n",
       "2  [\"To initialize the classification head when p...   \n",
       "\n",
       "                                     formatted_input  \\\n",
       "0  # **Based on the following prompt choose which...   \n",
       "1  # **Based on the following prompt choose which...   \n",
       "2  # **Based on the following prompt choose which...   \n",
       "\n",
       "                                                pred  \\\n",
       "0  ([winner_tie, winner_model_a, winner_model_b],...   \n",
       "1  ([winner_model_a, winner_model_b, winner_tie],...   \n",
       "2  ([winner_tie, winner_model_a, winner_model_b],...   \n",
       "\n",
       "                                  argsort_label                 argsort_prob  \n",
       "0  [winner_tie, winner_model_a, winner_model_b]   [0.6259, 0.22001, 0.20546]  \n",
       "1  [winner_model_a, winner_model_b, winner_tie]  [0.41876, 0.38439, 0.20498]  \n",
       "2  [winner_tie, winner_model_a, winner_model_b]  [0.55393, 0.35824, 0.14187]  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83a36d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['winner_tie', 'winner_model_a', 'winner_model_b'],\n",
       " array([0.77287, 0.6267 , 0.0885 ]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example = ''' \"Who the fuck are you? \n",
    "# his fee was an umberella it was a joke made by himself i have sources let me post em up it was on SKY SPORTS NEWS. \n",
    "# He was joking about the rain in manchester. So how the FUCK is that vandelising\" '''\n",
    "example = str(df_test.iloc[2])\n",
    "prediction(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4d740e",
   "metadata": {},
   "source": [
    "Using the prediction function with an example text demonstrates the reloaded model in action. The output shows the model's confidence scores for each category, sorted from most to least probable for the given input. This confirms that the saved adapters have been loaded correctly and the model is ready for inference tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc740861",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This comprehensive guide has walked you through a practical example of fine-tuning a large language model using LoRA adapters, a powerful technique for efficient customization. While the principles of LoRA are broadly applicable, we specifically tackled the challenge of adapting the promising (though, for this article’s context, hypothetical) Gemma 3 model for a multi-label sequence classification task — a scenario where direct high-level Hugging Face classes might not yet exist.\n",
    "\n",
    "By demonstrating how to add a custom classification head, wrap the model within a tailored PEFT-compatible class, manually ensure the integrity of adapter configurations, and navigate the nuances of training and prediction, this article aimed to equip you with both the general methodology and specific strategies needed. The journey from loading a base causal model to making multi-label predictions showcases the flexibility and potential that emerges when combining foundational LLM capabilities with targeted adaptation techniques. We hope this detailed exploration serves as a valuable blueprint for your own projects, empowering you to fine-tune cutting-edge models like Gemma 3 for a diverse array of sequence classification challenges.\n",
    "\n",
    "Please consider this script and the accompanying guide as a foundational example, not an exhaustively optimized solution. Please feel free experiment further: adjust the LoRA adapter configurations, refine the custom Python classes, tweak the Trainer hyperparameters, or even customize the loss function—indeed, explore any component you see fit! There are numerous parameters and architectural choices that can be fine-tuned to potentially achieve even better performance. This has been a comprehensive demonstration designed to provide you with a solid starting point for your own explorations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-project-test5)",
   "language": "python",
   "name": "didier-tests5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
