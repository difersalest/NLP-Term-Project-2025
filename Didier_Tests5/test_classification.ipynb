{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e351c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set the device to physical GPU 3\n",
    "# Physics server\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "\n",
    "env_path = \"./config/.env\"\n",
    "load_dotenv(dotenv_path=env_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce8d6e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "device_map = {\"\": 0}\n",
    "gpu_device   = 'cuda:0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b6881d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPUs available to PyTorch:\n",
      "----------------------------------------\n",
      "Device Index 0: NVIDIA A100-SXM4-80GB (79.25 GB)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_gpus = torch.cuda.device_count()\n",
    "print(f\"Found {num_gpus} GPUs available to PyTorch:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i in range(num_gpus):\n",
    "    name = torch.cuda.get_device_name(i)\n",
    "    mem = torch.cuda.get_device_properties(i).total_memory / 1024**3\n",
    "    print(f\"Device Index {i}: {name} ({mem:.2f} GB)\")\n",
    "\n",
    "print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8af7f8d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yishin/Didier/human_preference_pred/NLP-Term-Project-2025/Didier_Tests5/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from datasets import Dataset, load_dataset, DatasetDict \n",
    "\n",
    "from peft import (LoraConfig, \n",
    "                  PeftModel, \n",
    "                  prepare_model_for_kbit_training, \n",
    "                  get_peft_model,\n",
    "                  PeftModelForSequenceClassification,\n",
    "                  PeftConfig)\n",
    "\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification, \n",
    "    TrainingArguments, \n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    "    DataCollatorWithPadding,\n",
    "    AutoModelForCausalLM)\n",
    "\n",
    "import bitsandbytes as bnb\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff1188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proxy configured via IP address.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Configure the NTHU proxy directly in Python using the IP address\n",
    "proxy_url = \"http://140.114.63.4:3128\"\n",
    "\n",
    "os.environ['http_proxy'] = proxy_url\n",
    "os.environ['https_proxy'] = proxy_url\n",
    "os.environ['HTTP_PROXY'] = proxy_url\n",
    "os.environ['HTTPS_PROXY'] = proxy_url\n",
    "\n",
    "print(\"Proxy configured via IP address.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e93af30a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "hf_token = os.getenv('HF_TOKEN')\n",
    "login(hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0bfa0824",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_toxic = load_dataset(\"thesofakillers/jigsaw-toxic-comment-classification-challenge\")\n",
    "dataset_toxic = dataset_toxic['train']\n",
    "dataset_toxic = dataset_toxic.train_test_split(test_size=0.25,seed=42,)\n",
    "\n",
    "test_valid  = dataset_toxic['test'].train_test_split(test_size=0.5)\n",
    "\n",
    "dataset_toxic = DatasetDict({\n",
    "    'train': dataset_toxic['train'].select(range(1000)),\n",
    "    'valid': test_valid['train'].select(range(100)),\n",
    "    'test': test_valid['test'].select(range(100))})\n",
    "\n",
    "dataset_toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "45daf7b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5ccc42b72dd3f5db</td>\n",
       "      <td>Hello b.i.t.c.h \\nHello little s.l.u.t. Do you...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>aa6c40c32f92d39d</td>\n",
       "      <td>== Bad faith deletion of new article on Shefa ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8d113a1596e1d4d3</td>\n",
       "      <td>If you don't know what you're doing ... \\n\\nKe...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b93cd46037e2c08f</td>\n",
       "      <td>\"\\n\\nYou gave it away?! Why in the Sam Hill wo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9a135e8160fff88c</td>\n",
       "      <td>\"\\n\\n Popular culture \\n\\nA few months ago I p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  5ccc42b72dd3f5db  Hello b.i.t.c.h \\nHello little s.l.u.t. Do you...      1   \n",
       "1  aa6c40c32f92d39d  == Bad faith deletion of new article on Shefa ...      0   \n",
       "2  8d113a1596e1d4d3  If you don't know what you're doing ... \\n\\nKe...      0   \n",
       "3  b93cd46037e2c08f  \"\\n\\nYou gave it away?! Why in the Sam Hill wo...      0   \n",
       "4  9a135e8160fff88c  \"\\n\\n Popular culture \\n\\nA few months ago I p...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             1        1       0       1              1  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dataset_toxic['train'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "76fce763",
   "metadata": {},
   "outputs": [],
   "source": [
    "hugging_face_model_id = \"google/gemma-3-1b-it\" # google/gemma-3-4b-it\n",
    "\n",
    "from transformers import AutoTokenizer \n",
    "tokenizer = AutoTokenizer.from_pretrained(hugging_face_model_id,\n",
    "                                          padding_side='right',\n",
    "                                          device_map=device_map,\n",
    "                                          add_bos=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class2id = {'toxic':0,'severe_toxic':1,'obscene':2,'threat':3,'insult':4,'identity_hate':5}\n",
    "id2class = {v: k for k, v in class2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "756b8dc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1000/1000 [00:00<00:00, 2616.16 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2231.92 examples/s]\n",
      "Map: 100%|██████████| 100/100 [00:00<00:00, 2095.09 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_ids', 'attention_mask', 'labels'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_function(sample):\n",
    "    labels = []\n",
    "    for class_ in class2id.keys():\n",
    "        labels.append(sample[class_])\n",
    "\n",
    "    sample = tokenizer(sample['comment_text'], truncation=False)\n",
    "    sample['labels'] = labels\n",
    "    return sample\n",
    "\n",
    "\n",
    "dataset_toxic_tokenized = dataset_toxic.map(preprocess_function)\n",
    "dataset_toxic_tokenized = dataset_toxic_tokenized.select_columns(['input_ids','attention_mask','labels'])\n",
    "dataset_toxic_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4aefa63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input data for model:\n",
      "IDs   : [2, 236775, 108, 3048, 5877, 625, 3121, 26052, 8922, 528, 506, 6687, 10892, 1093, 611, 776, 600, 236881, 1452, 1537, 236789, 236745, 12141, 1003, 625, 236761, 38403, 236764, 141814, 3588, 855, 506, 76001, 49165, 564, 17231, 44395, 236764, 532, 564, 2752, 2506, 8379, 2802, 528, 1041, 14064, 236761, 1593, 625, 236789, 236751, 45320, 1041, 12866, 600, 692, 2752, 2506, 531, 1441, 1546, 1032, 568, 1452, 611, 236789, 560, 3931, 8672, 2311, 78345, 236881, 568, 3524, 735, 611, 1010, 528, 625, 672, 3697, 990, 236881, 5315, 990, 611, 236789, 560, 6950, 625, 2907, 2900, 573, 236881, 138, 240478, 201028, 138, 236775]\n",
      "Labels: [0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Input data decoded:\n",
      "Tokens: <bos>\"\n",
      "\n",
      "You gave it away?! Why in the Sam Hill would you do that? And don't worry about it. Actually, Brawl came out the VERY DAY I MOVED, and I never got internet access in my apartment. So it's kinda my fault that we never got to play each other ( And you've started college too huh? (Or have you been in it this whole time? First time you've mentioned it.) What for?  ► Complaints  \"\n",
      "Label dictionary: {'toxic': 0, 'severe_toxic': 0, 'obscene': 0, 'threat': 0, 'insult': 0, 'identity_hate': 0}\n"
     ]
    }
   ],
   "source": [
    "sample_index = 3 # Choose any sample index\n",
    "\n",
    "sample_input_ids = dataset_toxic_tokenized['train']['input_ids'][sample_index]\n",
    "sample_labels = dataset_toxic_tokenized['train']['labels'][sample_index]\n",
    "\n",
    "print('Input data for model:')\n",
    "print(f\"IDs   : {sample_input_ids}\")\n",
    "print(f\"Labels: {sample_labels}\\n\")\n",
    "\n",
    "print('Input data decoded:')\n",
    "print(f\"Tokens: {tokenizer.decode(sample_input_ids)}\")\n",
    "# Reconstruct the label dictionary for this sample\n",
    "decoded_labels = {id2class[i]: sample_labels[i] for i in range(len(sample_labels))}\n",
    "print(f\"Label dictionary: {decoded_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72ad58fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0454d489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 341, 37]\n",
      "[341, 341, 341]\n"
     ]
    }
   ],
   "source": [
    "sample_batch_ids           = dataset_toxic_tokenized['train']['input_ids'][0:3]\n",
    "sample_batch_ids_collator  = data_collator(dataset_toxic_tokenized['train'][:3])['input_ids'][0:3]\n",
    "print([len(x) for x in sample_batch_ids ])\n",
    "print([len(x) for x in sample_batch_ids_collator ])\n",
    "\n",
    "#length of each sample without datacollator : [74, 37, 159]\n",
    "#length of each sample with datacollator    :[159, 159, 159]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "45a0876e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "\n",
    "model = Gemma3ForCausalLM.from_pretrained(hugging_face_model_id, \n",
    "                                          torch_dtype=torch.bfloat16, \n",
    "                                          device_map=gpu_device,\n",
    "                                          attn_implementation='eager',\n",
    "                                          quantization_config=bnb_config  )\n",
    "\n",
    "model.lm_head = torch.nn.Linear(model.config.hidden_size, len(class2id.keys()), bias=False,device=gpu_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3bfa5215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training, get_peft_model\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "798585fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bitsandbytes as bnb\n",
    "def find_all_linear_names(model):\n",
    "    cls = bnb.nn.Linear4bit #if args.bits == 4 else (bnb.nn.Linear8bitLt if args.bits == 8 else torch.nn.Linear)\n",
    "    lora_module_names = set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, cls):\n",
    "            names = name.split('.')\n",
    "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
    "        if 'lm_head' in lora_module_names: # needed for 16-bit\n",
    "            lora_module_names.remove('lm_head')\n",
    "    return list(lora_module_names)\n",
    "\n",
    "modules = find_all_linear_names(model)\n",
    "modules = ['gate_proj', 'down_proj', 'v_proj', 'k_proj', 'q_proj', 'o_proj', 'up_proj']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "09a01a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\")\n",
    "\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e1c6edf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 52,183,040 || all params: 1,052,075,904 || trainable%: 4.9600\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()\n",
    "#trainable params: 119,209,984 || all params: 3,999,488,512 || trainable%: 2.9806"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1ca21f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Gemma3ForSequenceClassification(PeftModelForSequenceClassification):\n",
    "    def __init__(self, peft_config: PeftConfig, model: AutoModelForCausalLM, adapter_name=\"default\"):\n",
    "        super().__init__(model, peft_config, adapter_name)\n",
    "        self.num_labels = model.config.num_labels\n",
    "        self.problem_type = \"multi_label_classification\" # Assuming multi-label\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "        **kwargs):\n",
    "        \n",
    "        return_dict = return_dict if return_dict is not None else self.config.return_dict\n",
    "\n",
    "        outputs = self.base_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "            **kwargs)\n",
    "\n",
    "        # Extract logits from the outputs\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # select last \"real\" token and ignore padding tokens\n",
    "\n",
    "        sequence_lengths   = torch.sum(attention_mask, dim=1)\n",
    "        last_token_indices = sequence_lengths - 1\n",
    "        batch_size         = logits.shape[0]\n",
    "       \n",
    "        # Get the logits for the last token in the sequence\n",
    "        logits = logits[torch.arange(batch_size, device=logits.device), last_token_indices, :]\n",
    "        #logits = logits[:, -1, :] # if batch_size = 1\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.problem_type == \"regression\":\n",
    "                loss_fct = torch.nn.MSELoss()\n",
    "                loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "            elif self.problem_type == \"single_label_classification\":\n",
    "                loss_fct = torch.nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = torch.nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels.float())\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "21fb58d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = PeftConfig(peft_type=\"LORA\", task_type=\"SEQ_CLS\", inference_mode=False)\n",
    "for key, value in lora_config.__dict__.items():\n",
    "    setattr(peft_config, key, value)\n",
    "\n",
    "wrapped_model = Gemma3ForSequenceClassification(peft_config, model)\n",
    "wrapped_model.num_labels = len(class2id.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d947dd92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_binary_crossentropy_loss(logits, labels,epsilon=1e-7):\n",
    "  \n",
    "    probs = torch.sigmoid(logits)\n",
    "    probs = torch.clamp(probs, min=epsilon, max=1-epsilon) # capping values\n",
    "    loss  = -(labels * torch.log(probs) + (1 - labels) * torch.log(1 - probs))\n",
    "    return torch.mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "09d53b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):     \n",
    "    def compute_loss(self, model, inputs,num_items_in_batch=4, return_outputs=False): \n",
    "        labels  = inputs.get(\"labels\")\n",
    "        inputs  = inputs.to(gpu_device)\n",
    "        outputs = model(**inputs)\n",
    "        logits  = outputs.logits \n",
    "        \n",
    "        loss    = custom_binary_crossentropy_loss(logits, labels)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f300ed46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 6.79kB [00:00, 19.7MB/s]\n",
      "Downloading builder script: 7.56kB [00:00, 20.2MB/s]\n",
      "Downloading builder script: 7.38kB [00:00, 19.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "clf_metrics = evaluate.combine([\"accuracy\", \"f1\", \"precision\", \"recall\"])\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = sigmoid(predictions)\n",
    "    predictions = (predictions > 0.5).astype(int).reshape(-1)\n",
    "    return clf_metrics.compute(predictions=predictions, references=labels.astype(int).reshape(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "24326ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback, TrainingArguments, Trainer\n",
    "import os\n",
    "\n",
    "# Define early stopping and checkpoint directory\n",
    "early_stop = EarlyStoppingCallback(early_stopping_patience=3, # Increased patience slightly\n",
    "                                   early_stopping_threshold=0.001) # A small threshold\n",
    "checkpoints_dir = 'my_classification_gemma_model' # More descriptive name\n",
    "\n",
    "# Set tokenizer parallelism environment variable\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd590da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    gradient_checkpointing=False,  # Gradient Checkpointing ist nicht aktiviert\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=100,\n",
    "    #label_names=classes,\n",
    "    dataloader_num_workers=4,\n",
    "    output_dir= checkpoints_dir ,  # Output directory for checkpoints\n",
    "    learning_rate=5e-5,  # Learning rate for the optimizer\n",
    "    per_device_train_batch_size=8,  # Batch size per device\n",
    "    per_device_eval_batch_size=8,  # Batch size per device for evaluation \n",
    "    num_train_epochs=3,  # Number of training epochs\n",
    "    weight_decay=0.01,  # Weight decay for regularization\n",
    "    eval_strategy='epoch',  # Evaluate after each epoch\n",
    "    #eval_steps=100,\n",
    "    save_strategy=\"epoch\",  # Save model checkpoints after each epoch\n",
    "    load_best_model_at_end=True,  # Load the best model based on the chosen metric\n",
    "    push_to_hub=False,  # Disable pushing the model to the Hugging Face Hub \n",
    "    report_to=\"tensorboard\",  # Disable logging to Weight&Bias\n",
    "    logging_dir =  f\"tensorboard_my_model\",\n",
    "    gradient_accumulation_steps=4,\n",
    "    fp16=True,\n",
    "    warmup_ratio =0.05, \n",
    "    metric_for_best_model='eval_loss',)  # Metric for selecting the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cc898b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer (\n",
    "    model=wrapped_model,  # The LoRA-adapted model\n",
    "    args=training_args,  # Training arguments\n",
    "    train_dataset=dataset_toxic_tokenized['train'],  # Training dataset\n",
    "    eval_dataset=dataset_toxic_tokenized['valid'],  # Evaluation dataset\n",
    "    #tokenizer=tokenizer,  # Tokenizer for processing text\n",
    "    data_collator=data_collator,  # Data collator for preparing batches\n",
    "    compute_metrics=compute_metrics,  # Function to calculate evaluation metrics\n",
    "    callbacks=[early_stop]  # Optional early stopping callback\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa3decba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "/home/yishin/Didier/human_preference_pred/NLP-Term-Project-2025/Didier_Tests5/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='96' max='96' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [96/96 02:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.151279</td>\n",
       "      <td>0.961667</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.123017</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.105841</td>\n",
       "      <td>0.965000</td>\n",
       "      <td>0.322581</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.208333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yishin/Didier/human_preference_pred/NLP-Term-Project-2025/Didier_Tests5/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/home/yishin/Didier/human_preference_pred/NLP-Term-Project-2025/Didier_Tests5/.venv/lib/python3.11/site-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=96, training_loss=0.6360425551732382, metrics={'train_runtime': 173.4682, 'train_samples_per_second': 17.294, 'train_steps_per_second': 0.553, 'total_flos': 4003299077713920.0, 'train_loss': 0.6360425551732382, 'epoch': 3.0})"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train(resume_from_checkpoint=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "57c20e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(input_text):\n",
    "    inputs          = tokenizer(input_text, return_tensors=\"pt\",).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        outputs = wrapped_model(**inputs).logits\n",
    "    y_prob          = np.round(np.array(torch.sigmoid(outputs).tolist()[0]),5)\n",
    "    y_sorted_labels = [id2class.get(y) for y  in np.argsort(y_prob)[::-1]]\n",
    "    y_prob_sorted   = np.sort(y_prob)[::-1]\n",
    "    \n",
    "    return y_sorted_labels,y_prob_sorted  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8d127dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>pred</th>\n",
       "      <th>argsort_label</th>\n",
       "      <th>argsort_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>65546f3958775c00</td>\n",
       "      <td>No. You are not supposed to edit someone else'...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>([toxic, obscene, insult, severe_toxic, identi...</td>\n",
       "      <td>[toxic, obscene, insult, severe_toxic, identit...</td>\n",
       "      <td>[0.00146, 0.00043, 0.00028, 7e-05, 2e-05, 1e-05]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>03521147ef578854</td>\n",
       "      <td>Thumbing is when you put your thumb on his pen...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>([toxic, obscene, insult, severe_toxic, identi...</td>\n",
       "      <td>[toxic, obscene, insult, severe_toxic, identit...</td>\n",
       "      <td>[0.20979, 0.07893, 0.0278, 0.00381, 0.0027, 0....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  65546f3958775c00  No. You are not supposed to edit someone else'...      0   \n",
       "1  03521147ef578854  Thumbing is when you put your thumb on his pen...      1   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "\n",
       "                                                pred  \\\n",
       "0  ([toxic, obscene, insult, severe_toxic, identi...   \n",
       "1  ([toxic, obscene, insult, severe_toxic, identi...   \n",
       "\n",
       "                                       argsort_label  \\\n",
       "0  [toxic, obscene, insult, severe_toxic, identit...   \n",
       "1  [toxic, obscene, insult, severe_toxic, identit...   \n",
       "\n",
       "                                        argsort_prob  \n",
       "0   [0.00146, 0.00043, 0.00028, 7e-05, 2e-05, 1e-05]  \n",
       "1  [0.20979, 0.07893, 0.0278, 0.00381, 0.0027, 0....  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.DataFrame(dataset_toxic['test'])\n",
    "\n",
    "df_test['pred'] = df_test['comment_text'].map(prediction)\n",
    "df_test['argsort_label']  = df_test['pred'].apply(lambda x : x[0])\n",
    "df_test['argsort_prob']   = df_test['pred'].apply(lambda x : x[1])\n",
    "print(df_test.shape)\n",
    "df_test.head(n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "41f10405",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir =  f'my_awesome_model'\n",
    "trainer.model.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "21a5fa81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manually updated adapter configuration: my_awesome_model/adapter_config.json\n",
      "New content: {'auto_mapping': None, 'base_model_name_or_path': 'google/gemma-3-1b-it', 'inference_mode': True, 'peft_type': 'LORA', 'revision': None, 'task_type': 'SEQ_CLS', 'r': 64, 'lora_alpha': 32, 'lora_dropout': 0.1, 'target_modules': ['down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj'], 'bias': 'none', 'modules_to_save': ['classifier', 'score', 'classifier', 'score'], 'fan_in_fan_out': False, 'init_lora_weights': True}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "# Assuming lora_config is the LoraConfig object used during setup\n",
    "# Assuming hugging_face_model_id is the string ID like \"google/gemma-3-4b-it\"\n",
    "# Assuming output_dir is the path where the model was saved\n",
    "\n",
    "adapter_config_path = os.path.join(output_dir, \"adapter_config.json\")\n",
    "\n",
    "# Check if file exists before proceeding\n",
    "if os.path.exists(adapter_config_path):\n",
    "    try:\n",
    "        # Load the potentially incomplete config\n",
    "        with open(adapter_config_path, 'r') as f:\n",
    "            saved_config_dict = json.load(f)\n",
    "\n",
    "        # Get parameters from the original LoraConfig\n",
    "        # Use.to_dict() if available, otherwise __dict__\n",
    "        try:\n",
    "            # Ensure lora_config is the actual LoraConfig object instance\n",
    "            lora_config_dict = lora_config.to_dict()\n",
    "        except AttributeError:\n",
    "            # Fallback, might include extra internal attributes\n",
    "            lora_config_dict = lora_config.__dict__\n",
    "            # Clean up potential internal attributes if using __dict__\n",
    "            lora_config_dict = {k: v for k, v in lora_config_dict.items() if not k.startswith('_')}\n",
    "\n",
    "\n",
    "        # *** FIX 1: Define the specific keys to check ***\n",
    "        # These are common LoRA parameters that might be missing\n",
    "        lora_keys_to_check = [\n",
    "            \"r\",\n",
    "            \"lora_alpha\",\n",
    "            \"lora_dropout\",\n",
    "            \"target_modules\",\n",
    "            \"bias\",\n",
    "            \"modules_to_save\", # Important if you used it\n",
    "            \"fan_in_fan_out\",\n",
    "            \"init_lora_weights\",\n",
    "            # Add any other specific keys from your LoraConfig if needed\n",
    "        ]\n",
    "\n",
    "        # Merge missing or None parameters from the original lora_config\n",
    "        updated = False\n",
    "        for key in lora_keys_to_check:\n",
    "            # Check if key is missing in saved config OR if it exists but is None\n",
    "            if key not in saved_config_dict or saved_config_dict[key] is None:\n",
    "                # Check if the key exists in the original config and has a value\n",
    "                if key in lora_config_dict and lora_config_dict[key] is not None:\n",
    "                    saved_config_dict[key] = lora_config_dict[key]\n",
    "                    updated = True\n",
    "\n",
    "        # Ensure essential base fields are present and correct\n",
    "        # Use getattr for safer access to lora_config attributes\n",
    "        original_task_type = getattr(lora_config, 'task_type', 'SEQ_CLS')\n",
    "        if 'task_type' not in saved_config_dict or saved_config_dict['task_type']!= original_task_type:\n",
    "             saved_config_dict['task_type'] = original_task_type\n",
    "             updated = True\n",
    "\n",
    "        original_base_model = getattr(lora_config, 'base_model_name_or_path', hugging_face_model_id)\n",
    "        if 'base_model_name_or_path' not in saved_config_dict or saved_config_dict['base_model_name_or_path']!= original_base_model:\n",
    "             saved_config_dict['base_model_name_or_path'] = original_base_model\n",
    "             updated = True\n",
    "\n",
    "        if 'peft_type' not in saved_config_dict or saved_config_dict['peft_type']!= \"LORA\":\n",
    "             saved_config_dict['peft_type'] = \"LORA\"\n",
    "             updated = True\n",
    "\n",
    "        # *** FIX 2: Convert set to list before saving ***\n",
    "        if 'target_modules' in saved_config_dict and isinstance(saved_config_dict['target_modules'], set):\n",
    "            saved_config_dict['target_modules'] = sorted(list(saved_config_dict['target_modules'])) # Convert set to sorted list\n",
    "            updated = True # Mark as updated if conversion happened\n",
    "\n",
    "        if 'modules_to_save' in saved_config_dict and isinstance(saved_config_dict['modules_to_save'], set):\n",
    "             # Also handle modules_to_save if it could be a set\n",
    "             saved_config_dict['modules_to_save'] = sorted(list(saved_config_dict['modules_to_save']))\n",
    "             updated = True\n",
    "\n",
    "\n",
    "        # Overwrite the config file only if changes were made\n",
    "        if updated:\n",
    "            with open(adapter_config_path, 'w') as f:\n",
    "                # Save the corrected dictionary as JSON\n",
    "                json.dump(saved_config_dict, f, indent=2)\n",
    "            print(f\"Manually updated adapter configuration: {adapter_config_path}\")\n",
    "            print(\"New content:\", saved_config_dict) # Optional: print the final dict\n",
    "        else:\n",
    "            print(f\"Adapter configuration already seemed complete or no changes needed: {adapter_config_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during manual update of adapter_config.json: {e}\")\n",
    "else:\n",
    "    print(f\"Error: adapter_config.json not found at {adapter_config_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "360e9784",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir =  'my_awesome_model'\n",
    "hugging_face_model_id = \"google/gemma-3-1b-it\" # gemma-3-4b-it\n",
    "gpu_device = 'cuda:0'\n",
    "\n",
    "from transformers import AutoTokenizer \n",
    "import torch\n",
    "from transformers import AutoTokenizer, Gemma3ForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(hugging_face_model_id,\n",
    "                                          padding_side='right',\n",
    "                                          device_map=gpu_device,\n",
    "                                          add_bos=True)\n",
    "\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class2id = {'toxic':0,'severe_toxic':1,'obscene':2,'threat':3,'insult':4,'identity_hate':5}\n",
    "id2class = {v: k for k, v in class2id.items()}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16)\n",
    "\n",
    "base_model = Gemma3ForCausalLM.from_pretrained(hugging_face_model_id, \n",
    "                                          torch_dtype=torch.bfloat16, \n",
    "                                          device_map=gpu_device,\n",
    "                                          attn_implementation='eager',\n",
    "                                          quantization_config=bnb_config  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1cecd123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Replacing lm_head for 6 classes.\n"
     ]
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "modules = ['gate_proj', 'down_proj', 'v_proj', 'k_proj', 'q_proj', 'o_proj', 'up_proj']\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=64,\n",
    "    lora_alpha=32,\n",
    "    target_modules=modules,\n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=\"SEQ_CLS\",modules_to_save=['lm_head'])\n",
    "\n",
    "peft_config = PeftConfig(peft_type=\"LORA\", task_type=\"SEQ_CLS\", inference_mode=False)\n",
    "for key, value in lora_config.__dict__.items():\n",
    "    setattr(peft_config, key, value)\n",
    "\n",
    "\n",
    "num_labels = len(id2class.keys()) # Must match the number of classes used during training\n",
    "load_dtype = torch.bfloat16 # Match training or desired inference precision\n",
    "print(f\"Replacing lm_head for {num_labels} classes.\")\n",
    "base_model.lm_head = torch.nn.Linear(\n",
    "    base_model.config.hidden_size,\n",
    "    num_labels,\n",
    "    bias=False,\n",
    "    device=base_model.device # Ensure head is on the correct device\n",
    ").to(dtype=load_dtype) # Ensure head matches model dtype\n",
    "\n",
    "\n",
    "base_model = Gemma3ForSequenceClassification(peft_config, base_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3cadcffa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSequenceClassification(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Gemma3ForSequenceClassification(\n",
       "      (base_model): LoraModel(\n",
       "        (model): Gemma3ForCausalLM(\n",
       "          (model): Gemma3TextModel(\n",
       "            (embed_tokens): Gemma3TextScaledWordEmbedding(262144, 1152, padding_idx=0)\n",
       "            (layers): ModuleList(\n",
       "              (0-25): 26 x Gemma3DecoderLayer(\n",
       "                (self_attn): Gemma3Attention(\n",
       "                  (q_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=1024, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=1024, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (k_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (v_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=256, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=256, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (o_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1024, out_features=1152, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1024, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (q_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                  (k_norm): Gemma3RMSNorm((256,), eps=1e-06)\n",
       "                )\n",
       "                (mlp): Gemma3MLP(\n",
       "                  (gate_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=6912, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (up_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=1152, out_features=6912, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=1152, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=6912, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (down_proj): lora.Linear4bit(\n",
       "                    (base_layer): Linear4bit(in_features=6912, out_features=1152, bias=False)\n",
       "                    (lora_dropout): ModuleDict(\n",
       "                      (default): Dropout(p=0.1, inplace=False)\n",
       "                    )\n",
       "                    (lora_A): ModuleDict(\n",
       "                      (default): Linear(in_features=6912, out_features=64, bias=False)\n",
       "                    )\n",
       "                    (lora_B): ModuleDict(\n",
       "                      (default): Linear(in_features=64, out_features=1152, bias=False)\n",
       "                    )\n",
       "                    (lora_embedding_A): ParameterDict()\n",
       "                    (lora_embedding_B): ParameterDict()\n",
       "                    (lora_magnitude_vector): ModuleDict()\n",
       "                  )\n",
       "                  (act_fn): GELUTanh()\n",
       "                )\n",
       "                (input_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "                (post_attention_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "                (pre_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "                (post_feedforward_layernorm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "              )\n",
       "            )\n",
       "            (norm): Gemma3RMSNorm((1152,), eps=1e-06)\n",
       "            (rotary_emb): Gemma3RotaryEmbedding()\n",
       "            (rotary_emb_local): Gemma3RotaryEmbedding()\n",
       "          )\n",
       "          (lm_head): ModulesToSaveWrapper(\n",
       "            (original_module): Linear(in_features=1152, out_features=6, bias=False)\n",
       "            (modules_to_save): ModuleDict(\n",
       "              (default): Linear(in_features=1152, out_features=6, bias=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    output_dir,\n",
    "    is_trainable=False # Set to False for inference\n",
    ")\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "36842716",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(input_text):\n",
    "    inputs          = tokenizer(input_text, return_tensors=\"pt\",).to(\"cuda:0\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).logits\n",
    "    y_prob          = np.round(np.array(torch.sigmoid(outputs).tolist()[0]),5)\n",
    "    y_sorted_labels = [id2class.get(y) for y  in np.argsort(y_prob)[::-1]]\n",
    "    y_prob_sorted   = np.sort(y_prob)[::-1]\n",
    "    \n",
    "    return y_sorted_labels,y_prob_sorted "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "83a36d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['obscene', 'insult', 'toxic', 'severe_toxic', 'threat', 'identity_hate'],\n",
       " array([0.96484, 0.96094, 0.76953, 0.40625, 0.2793 , 0.08398]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = ''' \"Who the fuck are you? \n",
    "his fee was an umberella it was a joke made by himself i have sources let me post em up it was on SKY SPORTS NEWS. \n",
    "He was joking about the rain in manchester. So how the FUCK is that vandelising\" '''\n",
    "prediction(example)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp-project-test5)",
   "language": "python",
   "name": "didier-tests5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
